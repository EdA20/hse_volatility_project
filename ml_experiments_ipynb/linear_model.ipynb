{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29895fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/Samsung-IT-Academy/stepik-dl-nlp.git && pip install -r stepik-dl-nlp/requirements.txt\n",
    "import sys; sys.path.append('./stepik-dl-nlp')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm  \n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim import utils\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "\n",
    "# import dlnlputils\n",
    "# from dlnlputils.data import tokenize_text_simple_regex, tokenize_corpus, build_vocabulary, \\\n",
    "#     vectorize_texts, SparseFeaturesDataset\n",
    "# from dlnlputils.pipeline import train_eval_loop, predict_with_model, init_random_seed\n",
    "\n",
    "# init_random_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "129bab82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse\n",
    "import torch\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "023efcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "TOKEN_RE = re.compile(r'[\\w\\d]+')\n",
    "\n",
    "\n",
    "def tokenize_text_simple_regex(txt, min_token_size=4):\n",
    "    txt = txt.lower()\n",
    "    all_tokens = TOKEN_RE.findall(txt)\n",
    "    return [token for token in all_tokens if len(token) >= min_token_size]\n",
    "\n",
    "\n",
    "def character_tokenize(txt):\n",
    "    return list(txt)\n",
    "\n",
    "\n",
    "def tokenize_corpus(texts, tokenizer=tokenize_text_simple_regex, **tokenizer_kwargs):\n",
    "    return [tokenizer(text, **tokenizer_kwargs) for text in texts]\n",
    "\n",
    "\n",
    "def add_fake_token(word2id, token=''):\n",
    "    word2id_new = {token: i + 1 for token, i in word2id.items()}\n",
    "    word2id_new[token] = 0\n",
    "    return word2id_new\n",
    "\n",
    "\n",
    "def texts_to_token_ids(tokenized_texts, word2id):\n",
    "    return [[word2id[token] for token in text if token in word2id]\n",
    "            for text in tokenized_texts]\n",
    "\n",
    "\n",
    "def build_vocabulary(tokenized_texts, max_size=1000000, max_doc_freq=0.8, min_count=5, pad_word=None):\n",
    "    word_counts = collections.defaultdict(int)\n",
    "    doc_n = 0\n",
    "\n",
    "    # посчитать количество документов, в которых употребляется каждое слово\n",
    "    # а также общее количество документов\n",
    "    for txt in tokenized_texts:\n",
    "        doc_n += 1\n",
    "        unique_text_tokens = set(txt)\n",
    "        for token in unique_text_tokens:\n",
    "            word_counts[token] += 1\n",
    "\n",
    "    # убрать слишком редкие и слишком частые слова\n",
    "    word_counts = {word: cnt for word, cnt in word_counts.items()\n",
    "                   if cnt >= min_count and cnt / doc_n <= max_doc_freq}\n",
    "\n",
    "    # отсортировать слова по убыванию частоты\n",
    "    sorted_word_counts = sorted(word_counts.items(),\n",
    "                                reverse=True,\n",
    "                                key=lambda pair: pair[1])\n",
    "\n",
    "    # добавим несуществующее слово с индексом 0 для удобства пакетной обработки\n",
    "    if pad_word is not None:\n",
    "        sorted_word_counts = [(pad_word, 0)] + sorted_word_counts\n",
    "\n",
    "    # если у нас по прежнему слишком много слов, оставить только max_size самых частотных\n",
    "    if len(word_counts) > max_size:\n",
    "        sorted_word_counts = sorted_word_counts[:max_size]\n",
    "\n",
    "    # нумеруем слова\n",
    "    word2id = {word: i for i, (word, _) in enumerate(sorted_word_counts)}\n",
    "\n",
    "    # нормируем частоты слов\n",
    "    word2freq = np.array([cnt / doc_n for _, cnt in sorted_word_counts], dtype='float32')\n",
    "\n",
    "    return word2id, word2freq\n",
    "\n",
    "\n",
    "PAD_TOKEN = '__PAD__'\n",
    "NUMERIC_TOKEN = '__NUMBER__'\n",
    "NUMERIC_RE = re.compile(r'^([0-9.,e+\\-]+|[mcxvi]+)$', re.I)\n",
    "\n",
    "\n",
    "def replace_number_nokens(tokenized_texts):\n",
    "    return [[token if not NUMERIC_RE.match(token) else NUMERIC_TOKEN for token in text]\n",
    "            for text in tokenized_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2fae2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "def vectorize_texts(tokenized_texts, word2id, word2freq, mode='tfidf', scale=True):\n",
    "    assert mode in {'tfidf', 'idf', 'tf', 'bin'}\n",
    "\n",
    "    # считаем количество употреблений каждого слова в каждом документе\n",
    "    result = scipy.sparse.dok_matrix((len(tokenized_texts), len(word2id)), dtype='float32')\n",
    "    for text_i, text in enumerate(tokenized_texts):\n",
    "        for token in text:\n",
    "            if token in word2id:\n",
    "                result[text_i, word2id[token]] += 1\n",
    "\n",
    "    # получаем бинарные вектора \"встречается или нет\"\n",
    "    if mode == 'bin':\n",
    "        result = (result > 0).astype('float32')\n",
    "\n",
    "    # получаем вектора относительных частот слова в документе\n",
    "    elif mode == 'tf':\n",
    "        result = result.tocsr()\n",
    "        result = result.multiply(1 / result.sum(1))\n",
    "\n",
    "    # полностью убираем информацию о количестве употреблений слова в данном документе,\n",
    "    # но оставляем информацию о частотности слова в корпусе в целом\n",
    "    elif mode == 'idf':\n",
    "        result = (result > 0).astype('float32').multiply(1 / word2freq)\n",
    "\n",
    "    # учитываем всю информацию, которая у нас есть:\n",
    "    # частоту слова в документе и частоту слова в корпусе\n",
    "    elif mode == 'tfidf':\n",
    "        result = result.tocsr()\n",
    "        result = result.multiply(1 / result.sum(1))  # разделить каждую строку на её длину\n",
    "        result = result.multiply(1 / word2freq)  # разделить каждый столбец на вес слова\n",
    "\n",
    "    if scale:\n",
    "        result = result.tocsc()\n",
    "        result -= result.min()\n",
    "        result /= (result.max() + 1e-6)\n",
    "\n",
    "    return result.tocsr()\n",
    "\n",
    "\n",
    "class SparseFeaturesDataset(Dataset):\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.features.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        cur_features = torch.from_numpy(self.features[idx].toarray()[0]).float()\n",
    "        cur_label = torch.from_numpy(np.asarray(self.targets[idx])).long()\n",
    "        return cur_features, cur_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55a4ddab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymorphy2 import MorphAnalyzer\n",
    "from functools import lru_cache\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "m = MorphAnalyzer()\n",
    "regex = re.compile(\"[А-Яа-яA-z]+\")\n",
    "\n",
    "def words_only(text, regex=regex):\n",
    "    try:\n",
    "        return regex.findall(text.lower())\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "@lru_cache(maxsize=128)\n",
    "def lemmatize_word(token, pymorphy=m):\n",
    "    return pymorphy.parse(token)[0].normal_form\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatize_word(w) for w in text]\n",
    "\n",
    "\n",
    "mystopwords = stopwords.words('russian') \n",
    "def remove_stopwords(lemmas, stopwords = mystopwords):\n",
    "    return [w for w in lemmas if not w in stopwords and len(w) > 3]\n",
    "\n",
    "def clean_text(text):\n",
    "    tokens = words_only(text)\n",
    "    lemmas = lemmatize_text(tokens)\n",
    "    \n",
    "    return ' '.join(remove_stopwords(lemmas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "bb7c622e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "semeval-2016-2017-task3-subtaskBC\n",
      "semeval-2016-2017-task3-subtaskA-unannotated\n",
      "patent-2017\n",
      "quora-duplicate-questions\n",
      "wiki-english-20171001\n",
      "text8\n",
      "fake-news\n",
      "20-newsgroups\n",
      "__testing_matrix-synopsis\n",
      "__testing_multipart-matrix-synopsis\n",
      "fasttext-wiki-news-subwords-300\n",
      "conceptnet-numberbatch-17-06-300\n",
      "word2vec-ruscorpora-300\n",
      "word2vec-google-news-300\n",
      "glove-wiki-gigaword-50\n",
      "glove-wiki-gigaword-100\n",
      "glove-wiki-gigaword-200\n",
      "glove-wiki-gigaword-300\n",
      "glove-twitter-25\n",
      "glove-twitter-50\n",
      "glove-twitter-100\n",
      "glove-twitter-200\n",
      "__testing_word2vec-matrix-synopsis\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "for k, v in api.info().items():\n",
    "    for k1, v1 in v.items():\n",
    "        print(k1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9d2a1826",
   "metadata": {},
   "outputs": [],
   "source": [
    "from navec import Navec\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "path = 'navec_hudlit_v1_12B_500K_300d_100q.tar'\n",
    "navec = Navec.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1e41de57",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_t = api.load('glove-twitter-100')\n",
    "model_t1 = api.load('glove-twitter-200')\n",
    "model_t2 = api.load('glove-wiki-gigaword-200')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f447978e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import utils\n",
    "\n",
    "def get_vectors_gt100(row):\n",
    "    '''\n",
    "      word_doc_freq # частоты слов\n",
    "      train_tokenized #сами слова\n",
    "    '''\n",
    "#     vecs = [np.zeros(100)]\n",
    "    vecs = [np.zeros(300)]\n",
    "\n",
    "    for word in row:\n",
    "        #print(row)\n",
    "        try: \n",
    "            # если слово есть в нашем очищенном словаре\n",
    "            # умножаем вектор на вес tfidf\n",
    "            v = navec[word] * word_doc_freq[vocabulary[word]] \n",
    "#             v = model_t[word] * word_doc_freq[vocabulary[word]] \n",
    "\n",
    "        except:\n",
    "#             v = np.zeros(100)\n",
    "            v = np.zeros(300)\n",
    "        vecs.append(v)\n",
    "    return np.sum(np.array(vecs),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b180b4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import utils\n",
    "\n",
    "def get_vectors_gt100(row):\n",
    "    '''\n",
    "      word_doc_freq # частоты слов\n",
    "      train_tokenized #сами слова\n",
    "    '''\n",
    "    vecs = [np.zeros(100)]\n",
    "    for word in row:\n",
    "        #print(row)\n",
    "        try: \n",
    "            # если слово есть в нашем очищенном словаре\n",
    "            # умножаем вектор на вес tfidf\n",
    "            v = model_t[word] * word_doc_freq[vocabulary[word]] \n",
    "        except:\n",
    "            v = np.zeros(100)\n",
    "        vecs.append(v)\n",
    "    return np.sum(np.array(vecs),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "462cf8c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|███████▌                                                                           | 1/11 [00:00<00:05,  1.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4166666666666667\n",
      "данные: ['Alfa_Wealth.csv']\n",
      "всего наблюдений: 199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|███████████████                                                                    | 2/11 [00:08<00:46,  5.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.375\n",
      "данные: ['Alfa_Wealth.csv', 'bitkogan.csv']\n",
      "всего наблюдений: 2315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 27%|██████████████████████▋                                                            | 3/11 [00:21<01:08,  8.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5833333333333334\n",
      "данные: ['Alfa_Wealth.csv', 'bitkogan.csv', 'cbonds.csv']\n",
      "всего наблюдений: 4422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 36%|██████████████████████████████▏                                                    | 4/11 [00:32<01:05,  9.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "данные: ['Alfa_Wealth.csv', 'bitkogan.csv', 'cbonds.csv', 'headlines_QUANTS.csv']\n",
      "всего наблюдений: 4560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|█████████████████████████████████████▋                                             | 5/11 [00:46<01:06, 11.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7083333333333334\n",
      "данные: ['Alfa_Wealth.csv', 'bitkogan.csv', 'cbonds.csv', 'headlines_QUANTS.csv', 'mmi.csv']\n",
      "всего наблюдений: 6086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|█████████████████████████████████████████████▎                                     | 6/11 [01:06<01:10, 14.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.375\n",
      "данные: ['Alfa_Wealth.csv', 'bitkogan.csv', 'cbonds.csv', 'headlines_QUANTS.csv', 'mmi.csv', 'rshb_invest.csv']\n",
      "всего наблюдений: 7436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 64%|████████████████████████████████████████████████████▊                              | 7/11 [01:29<01:08, 17.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5416666666666666\n",
      "данные: ['Alfa_Wealth.csv', 'bitkogan.csv', 'cbonds.csv', 'headlines_QUANTS.csv', 'mmi.csv', 'rshb_invest.csv', 'signal.csv']\n",
      "всего наблюдений: 16313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 73%|████████████████████████████████████████████████████████████▎                      | 8/11 [01:50<00:55, 18.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5416666666666666\n",
      "данные: ['Alfa_Wealth.csv', 'bitkogan.csv', 'cbonds.csv', 'headlines_QUANTS.csv', 'mmi.csv', 'rshb_invest.csv', 'signal.csv', 'sky_bond.csv']\n",
      "всего наблюдений: 16421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 82%|███████████████████████████████████████████████████████████████████▉               | 9/11 [02:16<00:41, 20.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5416666666666666\n",
      "данные: ['Alfa_Wealth.csv', 'bitkogan.csv', 'cbonds.csv', 'headlines_QUANTS.csv', 'mmi.csv', 'rshb_invest.csv', 'signal.csv', 'sky_bond.csv', 'themovchans.csv']\n",
      "всего наблюдений: 16573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 91%|██████████████████████████████████████████████████████████████████████████▌       | 10/11 [02:37<00:20, 20.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.625\n",
      "данные: ['Alfa_Wealth.csv', 'bitkogan.csv', 'cbonds.csv', 'headlines_QUANTS.csv', 'mmi.csv', 'rshb_invest.csv', 'signal.csv', 'sky_bond.csv', 'themovchans.csv', 'vts.csv']\n",
      "всего наблюдений: 16735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [03:02<00:00, 16.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "данные: ['Alfa_Wealth.csv', 'bitkogan.csv', 'cbonds.csv', 'headlines_QUANTS.csv', 'mmi.csv', 'rshb_invest.csv', 'signal.csv', 'sky_bond.csv', 'themovchans.csv', 'vts.csv', 'War_Wealth_Wisdom.csv']\n",
      "всего наблюдений: 17025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "input_path = 'C:\\\\Users\\\\eduard\\\\jup\\\\project\\\\данные' # нужно указать путь до папки с данными по новостям\n",
    "\n",
    "for root, dirs, files in os.walk(input_path):\n",
    "    for q in tqdm(range(1, 12)):\n",
    "        df = pd.DataFrame([])\n",
    "        for file in files[:q]:\n",
    "            df1 = pd.read_csv(f'данные/{file}')\n",
    "            df = pd.concat([df, df1], axis=0, ignore_index=True)\n",
    "\n",
    "        df = df.sort_values('date', ignore_index=True)\n",
    "        df['date'] = pd.to_datetime(df['date']).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "        pnl = pd.read_csv('данные/zBacktest_EURUSD_5_days.txt')\n",
    "        pnl['date_start'] = pd.to_datetime(pnl['date_start']).dt.strftime('%Y-%m-%d')\n",
    "        pnl['pnl_sign'] = pnl['pnl'].apply(lambda x: 1 if x >= 0 else 0)\n",
    "        pnl['cumm_text'] = None\n",
    "\n",
    "        df_with_pnl = df[(df['date'] >= '2022-03-02') & (df['date'] <= '2022-11-01')]\n",
    "\n",
    "        i = 0\n",
    "        texts = ''\n",
    "        for _,row in df_with_pnl.iterrows():\n",
    "            if i >= pnl.shape[0]:\n",
    "                break\n",
    "\n",
    "            if row['date'] <= pnl['date_start'][i]:\n",
    "                texts += ' ' + row['text']\n",
    "            else:\n",
    "                pnl.at[i,'cumm_text'] = texts\n",
    "                texts = ''\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "        train, val = train_test_split(pnl)\n",
    "        train_tokenized = tokenize_corpus(train['cumm_text'])\n",
    "        val_tokenized = tokenize_corpus(val['cumm_text'])\n",
    "\n",
    "        MAX_DF = 0.8 #во скольких документах встречаеться слово\n",
    "        MIN_COUNT = 5 # сколько раз слово встречаеться в тексте\n",
    "\n",
    "\n",
    "        vocabulary, word_doc_freq = build_vocabulary(train_tokenized, max_doc_freq=MAX_DF, min_count=MIN_COUNT)\n",
    "        UNIQUE_WORDS_N = len(vocabulary)    \n",
    "        VECTORIZATION_MODE = 'tfidf'\n",
    "        train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n",
    "\n",
    "        train_vectors_gt100 = np.array([get_vectors_gt100(i) for i in train_tokenized])\n",
    "        val_vectors_gt100 = np.array([get_vectors_gt100(i) for i in val_tokenized])\n",
    "\n",
    "        model_t.most_similar(positive=['инвестор', 'рынок'], topn=1)\n",
    "\n",
    "        y_train = train['pnl_sign']\n",
    "        X_train = train_vectors_gt100\n",
    "\n",
    "        y_val = val['pnl_sign']\n",
    "        X_val = val_vectors_gt100\n",
    "\n",
    "        model = LogisticRegression()\n",
    "        model.fit(X_train, y_train)\n",
    "        print(model.score(X_val, y_val))\n",
    "        print('данные:', files[:q])\n",
    "        print('всего наблюдений:', df_with_pnl.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "3faf8196",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/11 [00:00<?, ?it/s]\n",
      "191it [00:00, 10402.43it/s]\n",
      "  9%|███████▌                                                                           | 1/11 [00:00<00:05,  1.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "данные: ['Alfa_Wealth.csv']\n",
      "всего наблюдений: 199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "629it [00:00, 6080.93it/s]\u001b[A\n",
      "1238it [00:00, 3290.57it/s]\u001b[A\n",
      "2236it [00:00, 4307.34it/s]\u001b[A\n",
      " 18%|███████████████                                                                    | 2/11 [00:07<00:36,  4.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.625\n",
      "данные: ['Alfa_Wealth.csv', 'bitkogan.csv']\n",
      "всего наблюдений: 2315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "962it [00:00, 9053.32it/s]\u001b[A\n",
      "1868it [00:00, 2301.28it/s]\u001b[A\n",
      "2913it [00:00, 3683.99it/s]\u001b[A\n",
      "4269it [00:01, 4095.89it/s]\u001b[A\n",
      " 27%|██████████████████████▋                                                            | 3/11 [00:18<00:59,  7.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.625\n",
      "данные: ['Alfa_Wealth.csv', 'bitkogan.csv', 'cbonds.csv']\n",
      "всего наблюдений: 4422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "961it [00:00, 9124.22it/s]\u001b[A\n",
      "1874it [00:00, 2286.94it/s]\u001b[A\n",
      "2976it [00:00, 3761.12it/s]\u001b[A\n",
      "4400it [00:01, 4096.09it/s]\u001b[A\n",
      " 36%|██████████████████████████████▏                                                    | 4/11 [00:29<01:02,  8.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4166666666666667\n",
      "данные: ['Alfa_Wealth.csv', 'bitkogan.csv', 'cbonds.csv', 'headlines_QUANTS.csv']\n",
      "всего наблюдений: 4560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1168it [00:00, 11393.09it/s]\u001b[A\n",
      "2308it [00:01, 2018.71it/s] \u001b[A\n",
      "2852it [00:01, 2254.47it/s]\u001b[A\n",
      "3886it [00:01, 3351.01it/s]\u001b[A\n",
      "5867it [00:01, 3572.88it/s]\u001b[A\n",
      " 45%|█████████████████████████████████████▋                                             | 5/11 [00:45<01:08, 11.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5416666666666666\n",
      "данные: ['Alfa_Wealth.csv', 'bitkogan.csv', 'cbonds.csv', 'headlines_QUANTS.csv', 'mmi.csv']\n",
      "всего наблюдений: 6086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1386it [00:00, 12247.05it/s]\u001b[A\n",
      "2611it [00:01, 2077.26it/s] \u001b[A\n",
      "3190it [00:01, 1594.82it/s]\u001b[A\n",
      "4499it [00:01, 2700.18it/s]\u001b[A\n",
      "5211it [00:02, 2588.55it/s]\u001b[A\n",
      "7169it [00:02, 2979.89it/s]\u001b[A\n",
      " 55%|█████████████████████████████████████████████▎                                     | 6/11 [01:04<01:10, 14.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "данные: ['Alfa_Wealth.csv', 'bitkogan.csv', 'cbonds.csv', 'headlines_QUANTS.csv', 'mmi.csv', 'rshb_invest.csv']\n",
      "всего наблюдений: 7436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1504it [00:00, 14509.29it/s]\u001b[A\n",
      "3064it [00:00, 14667.94it/s]\u001b[A\n",
      "4531it [00:00, 5606.44it/s] \u001b[A\n",
      "5473it [00:01, 3205.39it/s]\u001b[A\n",
      "6105it [00:01, 2242.74it/s]\u001b[A\n",
      "6548it [00:02, 1841.83it/s]\u001b[A\n",
      "6875it [00:02, 1576.66it/s]\u001b[A\n",
      "7125it [00:02, 1649.52it/s]\u001b[A\n",
      "8512it [00:02, 3139.53it/s]\u001b[A\n",
      "10103it [00:02, 4926.01it/s]\u001b[A\n",
      "10943it [00:03, 4416.43it/s]\u001b[A\n",
      "11630it [00:03, 3417.59it/s]\u001b[A\n",
      "12168it [00:03, 3376.16it/s]\u001b[A\n",
      "13752it [00:03, 5301.77it/s]\u001b[A\n",
      "15726it [00:03, 3943.92it/s]\u001b[A\n",
      " 64%|████████████████████████████████████████████████████▊                              | 7/11 [01:24<01:03, 15.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4583333333333333\n",
      "данные: ['Alfa_Wealth.csv', 'bitkogan.csv', 'cbonds.csv', 'headlines_QUANTS.csv', 'mmi.csv', 'rshb_invest.csv', 'signal.csv']\n",
      "всего наблюдений: 16313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1523it [00:00, 13591.97it/s]\u001b[A\n",
      "3088it [00:00, 14477.28it/s]\u001b[A\n",
      "4539it [00:00, 5521.89it/s] \u001b[A\n",
      "5469it [00:01, 3267.24it/s]\u001b[A\n",
      "6096it [00:01, 2294.37it/s]\u001b[A\n",
      "6537it [00:02, 1845.35it/s]\u001b[A\n",
      "6861it [00:02, 1540.14it/s]\u001b[A\n",
      "7106it [00:02, 1503.00it/s]\u001b[A\n",
      "8546it [00:02, 2944.91it/s]\u001b[A\n",
      "10093it [00:03, 4647.78it/s]\u001b[A\n",
      "10941it [00:03, 4259.08it/s]\u001b[A\n",
      "11635it [00:03, 3346.12it/s]\u001b[A\n",
      "12178it [00:03, 3226.87it/s]\u001b[A\n",
      "13701it [00:03, 4983.53it/s]\u001b[A\n",
      "15828it [00:04, 3867.41it/s]\u001b[A\n",
      " 73%|████████████████████████████████████████████████████████████▎                      | 8/11 [01:46<00:53, 17.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7083333333333334\n",
      "данные: ['Alfa_Wealth.csv', 'bitkogan.csv', 'cbonds.csv', 'headlines_QUANTS.csv', 'mmi.csv', 'rshb_invest.csv', 'signal.csv', 'sky_bond.csv']\n",
      "всего наблюдений: 16421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1342it [00:00, 13134.70it/s]\u001b[A\n",
      "2971it [00:00, 14367.62it/s]\u001b[A\n",
      "4406it [00:00, 5671.23it/s] \u001b[A\n",
      "5333it [00:01, 3296.39it/s]\u001b[A\n",
      "5959it [00:01, 2386.18it/s]\u001b[A\n",
      "6403it [00:02, 1930.34it/s]\u001b[A\n",
      "6731it [00:02, 1573.47it/s]\u001b[A\n",
      "6978it [00:02, 1382.94it/s]\u001b[A\n",
      "7497it [00:02, 1770.89it/s]\u001b[A\n",
      "8928it [00:03, 3401.68it/s]\u001b[A\n",
      "10249it [00:03, 4878.34it/s]\u001b[A\n",
      "11067it [00:03, 4237.39it/s]\u001b[A\n",
      "11731it [00:03, 3196.03it/s]\u001b[A\n",
      "12249it [00:04, 3083.24it/s]\u001b[A\n",
      "13743it [00:04, 4858.57it/s]\u001b[A\n",
      "15960it [00:04, 3731.58it/s]\u001b[A\n",
      " 82%|███████████████████████████████████████████████████████████████████▉               | 9/11 [02:11<00:40, 20.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5416666666666666\n",
      "данные: ['Alfa_Wealth.csv', 'bitkogan.csv', 'cbonds.csv', 'headlines_QUANTS.csv', 'mmi.csv', 'rshb_invest.csv', 'signal.csv', 'sky_bond.csv', 'themovchans.csv']\n",
      "всего наблюдений: 16573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1515it [00:00, 14522.98it/s]\u001b[A\n",
      "3148it [00:00, 15082.02it/s]\u001b[A\n",
      "4656it [00:00, 5207.23it/s] \u001b[A\n",
      "5607it [00:01, 3040.64it/s]\u001b[A\n",
      "6241it [00:01, 2200.24it/s]\u001b[A\n",
      "6686it [00:02, 1793.12it/s]\u001b[A\n",
      "7013it [00:02, 1503.42it/s]\u001b[A\n",
      "7259it [00:02, 1527.27it/s]\u001b[A\n",
      "8647it [00:03, 2918.11it/s]\u001b[A\n",
      "10159it [00:03, 4602.25it/s]\u001b[A\n",
      "11017it [00:03, 4284.21it/s]\u001b[A\n",
      "11723it [00:03, 3367.11it/s]\u001b[A\n",
      "12275it [00:04, 2964.29it/s]\u001b[A\n",
      "13761it [00:04, 4586.97it/s]\u001b[A\n",
      "16119it [00:04, 3757.14it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████████████████████████████████████████▌       | 10/11 [02:34<00:20, 20.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5416666666666666\n",
      "данные: ['Alfa_Wealth.csv', 'bitkogan.csv', 'cbonds.csv', 'headlines_QUANTS.csv', 'mmi.csv', 'rshb_invest.csv', 'signal.csv', 'sky_bond.csv', 'themovchans.csv', 'vts.csv']\n",
      "всего наблюдений: 16735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1270it [00:00, 11457.90it/s]\u001b[A\n",
      "2771it [00:00, 13277.52it/s]\u001b[A\n",
      "4105it [00:00, 7098.52it/s] \u001b[A\n",
      "5035it [00:01, 3818.33it/s]\u001b[A\n",
      "5672it [00:01, 2699.38it/s]\u001b[A\n",
      "6129it [00:01, 2045.11it/s]\u001b[A\n",
      "6465it [00:02, 1694.43it/s]\u001b[A\n",
      "6721it [00:02, 1507.97it/s]\u001b[A\n",
      "6926it [00:02, 1309.79it/s]\u001b[A\n",
      "7090it [00:03, 1168.42it/s]\u001b[A\n",
      "7226it [00:03, 1069.13it/s]\u001b[A\n",
      "7777it [00:03, 1693.05it/s]\u001b[A\n",
      "9219it [00:03, 3832.40it/s]\u001b[A\n",
      "10531it [00:03, 5612.52it/s]\u001b[A\n",
      "11344it [00:03, 4455.36it/s]\u001b[A\n",
      "12000it [00:04, 3187.70it/s]\u001b[A\n",
      "12510it [00:04, 2822.40it/s]\u001b[A\n",
      "13847it [00:04, 4354.81it/s]\u001b[A\n",
      "16401it [00:04, 3453.40it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [02:54<00:00, 15.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "данные: ['Alfa_Wealth.csv', 'bitkogan.csv', 'cbonds.csv', 'headlines_QUANTS.csv', 'mmi.csv', 'rshb_invest.csv', 'signal.csv', 'sky_bond.csv', 'themovchans.csv', 'vts.csv', 'War_Wealth_Wisdom.csv']\n",
      "всего наблюдений: 17025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "input_path = 'C:\\\\Users\\\\eduard\\\\jup\\\\project\\\\данные'\n",
    "\n",
    "for root, dirs, files in os.walk(input_path):\n",
    "    for q in tqdm(range(1, 12)):\n",
    "        df = pd.DataFrame([])\n",
    "        for file in files[:q]:\n",
    "            df1 = pd.read_csv(f'данные/{file}')\n",
    "            df = pd.concat([df, df1], axis=0, ignore_index=True)\n",
    "\n",
    "        df = df.sort_values('date', ignore_index=True)\n",
    "        df['date'] = pd.to_datetime(df['date']).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "        pnl = pd.read_csv('данные/zBacktest_EURUSD_5_days.txt')\n",
    "        pnl['date_start'] = pd.to_datetime(pnl['date_start']).dt.strftime('%Y-%m-%d')\n",
    "        pnl['pnl_sign'] = pnl['pnl'].apply(lambda x: 1 if x >= 0 else 0)\n",
    "        pnl['cumm_text'] = None\n",
    "\n",
    "        df_with_pnl = df[(df['date'] >= '2022-03-02') & (df['date'] <= '2022-11-01')]\n",
    "\n",
    "        i = 0\n",
    "        texts = ''\n",
    "        for _,row in tqdm(df_with_pnl.iterrows()):\n",
    "            if i >= pnl.shape[0]:\n",
    "                break\n",
    "\n",
    "            if row['date'] <= pnl['date_start'][i]:\n",
    "                texts += ' ' + row['text']\n",
    "            else:\n",
    "                pnl.at[i,'cumm_text'] = texts\n",
    "                texts = ''\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "        train, val = train_test_split(pnl)\n",
    "        train_tokenized = tokenize_corpus(train['cumm_text'])\n",
    "        val_tokenized = tokenize_corpus(val['cumm_text'])\n",
    "\n",
    "        MAX_DF = 0.8 #во скольких документах встречаеться слово\n",
    "        MIN_COUNT = 5 # сколько раз слово встречаеться в тексте\n",
    "\n",
    "\n",
    "        vocabulary, word_doc_freq = build_vocabulary(train_tokenized, max_doc_freq=MAX_DF, min_count=MIN_COUNT)\n",
    "        UNIQUE_WORDS_N = len(vocabulary)    \n",
    "        VECTORIZATION_MODE = 'tfidf'\n",
    "        train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n",
    "\n",
    "        train_vectors_gt100 = np.array([get_vectors_gt100(i) for i in train_tokenized])\n",
    "        val_vectors_gt100 = np.array([get_vectors_gt100(i) for i in val_tokenized])\n",
    "\n",
    "        model_t.most_similar(positive=['инвестор', 'рынок'], topn=1)\n",
    "\n",
    "        y_train = train['pnl_sign']\n",
    "        X_train = train_vectors_gt100\n",
    "\n",
    "        y_val = val['pnl_sign']\n",
    "        X_val = val_vectors_gt100\n",
    "\n",
    "        model = LogisticRegression()\n",
    "        model.fit(X_train, y_train)\n",
    "        print(model.score(X_val, y_val))\n",
    "        print('данные:', files[:q])\n",
    "        print('всего наблюдений:', df_with_pnl.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89427951",
   "metadata": {},
   "source": [
    "# Новый"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7b322452",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.19it/s]\n"
     ]
    }
   ],
   "source": [
    "input_path = 'C:\\\\Users\\\\eduard\\\\jup\\\\project\\\\данные'\n",
    "\n",
    "df = pd.DataFrame([])\n",
    "for root, dirs, files in os.walk(input_path):\n",
    "    for file in tqdm(files[:5]):\n",
    "        df1 = pd.read_csv(f'данные\\{file}')\n",
    "        df = pd.concat([df, df1], axis=0, ignore_index=True)\n",
    "\n",
    "df = df.sort_values('date', ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b97bf1a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_start</th>\n",
       "      <th>pnl</th>\n",
       "      <th>pnl_sign</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-01-04</td>\n",
       "      <td>291264.097914</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-01-05</td>\n",
       "      <td>-411993.830320</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-01-06</td>\n",
       "      <td>521491.686795</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-01-08</td>\n",
       "      <td>62842.634116</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-01-11</td>\n",
       "      <td>-537598.706217</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>2021-12-20</td>\n",
       "      <td>-407527.554561</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>2021-12-21</td>\n",
       "      <td>-176881.417077</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>2021-12-22</td>\n",
       "      <td>-206943.414418</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>2021-12-23</td>\n",
       "      <td>23073.596468</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>2021-12-24</td>\n",
       "      <td>283960.438531</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     date_start            pnl  pnl_sign\n",
       "0    2021-01-04  291264.097914         1\n",
       "1    2021-01-05 -411993.830320         0\n",
       "2    2021-01-06  521491.686795         1\n",
       "3    2021-01-08   62842.634116         1\n",
       "4    2021-01-11 -537598.706217         0\n",
       "..          ...            ...       ...\n",
       "245  2021-12-20 -407527.554561         0\n",
       "246  2021-12-21 -176881.417077         0\n",
       "247  2021-12-22 -206943.414418         0\n",
       "248  2021-12-23   23073.596468         1\n",
       "249  2021-12-24  283960.438531         1\n",
       "\n",
       "[250 rows x 3 columns]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pnl = pd.read_csv('данные/zBacktest_USDRUB_5_days.txt')\n",
    "pnl['date_start'] = pd.to_datetime(pnl['date_start']).dt.strftime('%Y-%m-%d')\n",
    "pnl['pnl_sign'] = pnl['pnl'].apply(lambda x: 1 if x >= 0 else 0)\n",
    "pnl.sort_values('date_start')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3d7e6ef3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_start</th>\n",
       "      <th>pnl</th>\n",
       "      <th>pnl_sign</th>\n",
       "      <th>cumm_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-01-04</td>\n",
       "      <td>291264.097914</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-01-05</td>\n",
       "      <td>-411993.830320</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-01-06</td>\n",
       "      <td>521491.686795</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-01-08</td>\n",
       "      <td>62842.634116</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-01-11</td>\n",
       "      <td>-537598.706217</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>2021-12-20</td>\n",
       "      <td>-407527.554561</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>2021-12-21</td>\n",
       "      <td>-176881.417077</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>2021-12-22</td>\n",
       "      <td>-206943.414418</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>2021-12-23</td>\n",
       "      <td>23073.596468</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>2021-12-24</td>\n",
       "      <td>283960.438531</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     date_start            pnl  pnl_sign cumm_text\n",
       "0    2021-01-04  291264.097914         1      None\n",
       "1    2021-01-05 -411993.830320         0      None\n",
       "2    2021-01-06  521491.686795         1      None\n",
       "3    2021-01-08   62842.634116         1      None\n",
       "4    2021-01-11 -537598.706217         0      None\n",
       "..          ...            ...       ...       ...\n",
       "245  2021-12-20 -407527.554561         0      None\n",
       "246  2021-12-21 -176881.417077         0      None\n",
       "247  2021-12-22 -206943.414418         0      None\n",
       "248  2021-12-23   23073.596468         1      None\n",
       "249  2021-12-24  283960.438531         1      None\n",
       "\n",
       "[250 rows x 4 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pnl['cumm_text'] = None\n",
    "pnl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "15596d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['date']).dt.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "07039881",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_with_pnl = df[(df['date'] >= '2021-01-04') & (df['date'] <= '2021-12-24')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "56f9c784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_start</th>\n",
       "      <th>pnl</th>\n",
       "      <th>pnl_sign</th>\n",
       "      <th>cumm_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-01-04</td>\n",
       "      <td>291264.097914</td>\n",
       "      <td>1</td>\n",
       "      <td>🟢 - настроения risk-on доминируют в начале нас...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-01-05</td>\n",
       "      <td>-411993.830320</td>\n",
       "      <td>0</td>\n",
       "      <td>🔴 - умеренно-негативные настроения с утра: рос...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-01-06</td>\n",
       "      <td>521491.686795</td>\n",
       "      <td>1</td>\n",
       "      <td>ВСЕМИРНЫЙ БАНК ПОВЫСИЛ ПРОГНОЗЫ, КОТОРЫЕ, ТЕМ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-01-08</td>\n",
       "      <td>62842.634116</td>\n",
       "      <td>1</td>\n",
       "      <td>НОВЫЙ РЕКОРД ПО ЧИСЛУ ЗАРАЗИВШИХСЯ В МИРЕ За п...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-01-11</td>\n",
       "      <td>-537598.706217</td>\n",
       "      <td>0</td>\n",
       "      <td>РУБЛЕВЫЙ ИНДЕКС МИРОВЫХ ЦЕН НА ПРОДОВОЛЬСТВИЕ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>2021-12-20</td>\n",
       "      <td>-407527.554561</td>\n",
       "      <td>0</td>\n",
       "      <td>Что ожидать инвестору от предстоящей недели. Д...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>2021-12-21</td>\n",
       "      <td>-176881.417077</td>\n",
       "      <td>0</td>\n",
       "      <td>РЫНОК УВЕРЕН, ЧТО ВЫШЕ 7% КЛЮЧЕВАЯ СТАВКА НЕ П...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>2021-12-22</td>\n",
       "      <td>-206943.414418</td>\n",
       "      <td>0</td>\n",
       "      <td>•  Обвал криптовалют не привёл к сколь-либо за...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>2021-12-23</td>\n",
       "      <td>23073.596468</td>\n",
       "      <td>1</td>\n",
       "      <td>•   Над Бразилией продолжают сгущаться тучи   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>2021-12-24</td>\n",
       "      <td>283960.438531</td>\n",
       "      <td>1</td>\n",
       "      <td>США ВВОДЯТ ОБЯЗАТЕЛЬНУЮ ВАКЦИНАЦИЮ ДЛЯ ОТДЕЛЬН...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     date_start            pnl  pnl_sign  \\\n",
       "0    2021-01-04  291264.097914         1   \n",
       "1    2021-01-05 -411993.830320         0   \n",
       "2    2021-01-06  521491.686795         1   \n",
       "3    2021-01-08   62842.634116         1   \n",
       "4    2021-01-11 -537598.706217         0   \n",
       "..          ...            ...       ...   \n",
       "245  2021-12-20 -407527.554561         0   \n",
       "246  2021-12-21 -176881.417077         0   \n",
       "247  2021-12-22 -206943.414418         0   \n",
       "248  2021-12-23   23073.596468         1   \n",
       "249  2021-12-24  283960.438531         1   \n",
       "\n",
       "                                             cumm_text  \n",
       "0    🟢 - настроения risk-on доминируют в начале нас...  \n",
       "1    🔴 - умеренно-негативные настроения с утра: рос...  \n",
       "2    ВСЕМИРНЫЙ БАНК ПОВЫСИЛ ПРОГНОЗЫ, КОТОРЫЕ, ТЕМ ...  \n",
       "3    НОВЫЙ РЕКОРД ПО ЧИСЛУ ЗАРАЗИВШИХСЯ В МИРЕ За п...  \n",
       "4    РУБЛЕВЫЙ ИНДЕКС МИРОВЫХ ЦЕН НА ПРОДОВОЛЬСТВИЕ ...  \n",
       "..                                                 ...  \n",
       "245  Что ожидать инвестору от предстоящей недели. Д...  \n",
       "246  РЫНОК УВЕРЕН, ЧТО ВЫШЕ 7% КЛЮЧЕВАЯ СТАВКА НЕ П...  \n",
       "247  •  Обвал криптовалют не привёл к сколь-либо за...  \n",
       "248  •   Над Бразилией продолжают сгущаться тучи   ...  \n",
       "249  США ВВОДЯТ ОБЯЗАТЕЛЬНУЮ ВАКЦИНАЦИЮ ДЛЯ ОТДЕЛЬН...  \n",
       "\n",
       "[250 rows x 4 columns]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pnl['cumm_text'] = df_with_pnl.groupby('date', as_index = False).agg({'text': ' '.join})['text']\n",
    "pnl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e898c5c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 187/187 [01:31<00:00,  2.05it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 63/63 [00:32<00:00,  1.92it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, val = train_test_split(pnl)\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "train['lemmas'] = train['cumm_text'].progress_apply(lambda x: clean_text(x))\n",
    "val['lemmas'] = val['cumm_text'].progress_apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "04bee485",
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = ColumnTransformer([\n",
    "    ('vec', TfidfVectorizer(ngram_range=(1, 2), max_df = 0.95, min_df = 0.001), 'lemmas')\n",
    "    ])\n",
    "\n",
    "train_tr = ct.fit_transform(train)\n",
    "clf = LogisticRegression(random_state=42, max_iter=500, warm_start=True)\n",
    "\n",
    "y_train = train['pnl_sign'].values\n",
    "clf.fit(train_tr, y_train)\n",
    "\n",
    "test_data = ct.transform(val)\n",
    "pred = clf.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bed8dd35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5873015873015873\n"
     ]
    }
   ],
   "source": [
    "train_tokenized = tokenize_corpus(train['lemmas'])\n",
    "val_tokenized = tokenize_corpus(val['lemmas'])\n",
    "\n",
    "MAX_DF = 0.8 #во скольких документах встречаеться слово\n",
    "MIN_COUNT = 5 # сколько раз слово встречаеться в тексте\n",
    "\n",
    "\n",
    "vocabulary, word_doc_freq = build_vocabulary(train_tokenized, max_doc_freq=MAX_DF, min_count=MIN_COUNT)\n",
    "UNIQUE_WORDS_N = len(vocabulary)    \n",
    "VECTORIZATION_MODE = 'tfidf'\n",
    "train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n",
    "\n",
    "train_vectors_gt100 = np.array([get_vectors_gt100(i) for i in train_tokenized])\n",
    "val_vectors_gt100 = np.array([get_vectors_gt100(i) for i in val_tokenized])\n",
    "\n",
    "model_t.most_similar(positive=['инвестор', 'рынок'], topn=1)\n",
    "\n",
    "y_train = train['pnl_sign']\n",
    "X_train = train_vectors_gt100\n",
    "\n",
    "y_val = val['pnl_sign']\n",
    "X_val = val_vectors_gt100\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "print(model.score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f6a7d5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5238095238095238\n"
     ]
    }
   ],
   "source": [
    "# train_tokenized = tokenize_corpus(train['lemmas'])\n",
    "# val_tokenized = tokenize_corpus(val['lemmas'])\n",
    "train_tokenized = tokenize_corpus(train['cumm_text'])\n",
    "val_tokenized = tokenize_corpus(val['cumm_text'])\n",
    "\n",
    "MAX_DF = 0.8 #во скольких документах встречаеться слово\n",
    "MIN_COUNT = 5 # сколько раз слово встречаеться в тексте\n",
    "\n",
    "\n",
    "vocabulary, word_doc_freq = build_vocabulary(train_tokenized, max_doc_freq=MAX_DF, min_count=MIN_COUNT)\n",
    "UNIQUE_WORDS_N = len(vocabulary)    \n",
    "VECTORIZATION_MODE = 'tfidf'\n",
    "train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n",
    "\n",
    "train_vectors_gt100 = np.array([get_vectors_gt100(i) for i in train_tokenized])\n",
    "val_vectors_gt100 = np.array([get_vectors_gt100(i) for i in val_tokenized])\n",
    "\n",
    "model_t.most_similar(positive=['инвестор', 'рынок'], topn=1)\n",
    "\n",
    "y_train = train['pnl_sign']\n",
    "X_train = train_vectors_gt100\n",
    "\n",
    "y_val = val['pnl_sign']\n",
    "X_val = val_vectors_gt100\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "print(model.score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d0a1b5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47619047619047616\n"
     ]
    }
   ],
   "source": [
    "train_tokenized = tokenize_corpus(train['lemmas'])\n",
    "val_tokenized = tokenize_corpus(val['lemmas'])\n",
    "# train_tokenized = tokenize_corpus(train['cumm_text'])\n",
    "# val_tokenized = tokenize_corpus(val['cumm_text'])\n",
    "\n",
    "MAX_DF = 0.8 #во скольких документах встречаеться слово\n",
    "MIN_COUNT = 5 # сколько раз слово встречаеться в тексте\n",
    "\n",
    "\n",
    "vocabulary, word_doc_freq = build_vocabulary(train_tokenized, max_doc_freq=MAX_DF, min_count=MIN_COUNT)\n",
    "UNIQUE_WORDS_N = len(vocabulary)    \n",
    "VECTORIZATION_MODE = 'tfidf'\n",
    "train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n",
    "\n",
    "train_vectors_gt100 = np.array([get_vectors_gt100(i) for i in train_tokenized])\n",
    "val_vectors_gt100 = np.array([get_vectors_gt100(i) for i in val_tokenized])\n",
    "\n",
    "model_t.most_similar(positive=['инвестор', 'рынок'], topn=1)\n",
    "\n",
    "y_train = train['pnl_sign']\n",
    "X_train = train_vectors_gt100\n",
    "\n",
    "y_val = val['pnl_sign']\n",
    "X_val = val_vectors_gt100\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "print(model.score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a393dc91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4583333333333333\n"
     ]
    }
   ],
   "source": [
    "train_tokenized = tokenize_corpus(train['cumm_text'])\n",
    "val_tokenized = tokenize_corpus(val['cumm_text'])\n",
    "\n",
    "MAX_DF = 0.8 #во скольких документах встречаеться слово\n",
    "MIN_COUNT = 5 # сколько раз слово встречаеться в тексте\n",
    "\n",
    "\n",
    "vocabulary, word_doc_freq = build_vocabulary(train_tokenized, max_doc_freq=MAX_DF, min_count=MIN_COUNT)\n",
    "UNIQUE_WORDS_N = len(vocabulary)    \n",
    "VECTORIZATION_MODE = 'tfidf'\n",
    "train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n",
    "\n",
    "train_vectors_gt100 = np.array([get_vectors_gt100(i) for i in train_tokenized])\n",
    "val_vectors_gt100 = np.array([get_vectors_gt100(i) for i in val_tokenized])\n",
    "\n",
    "model_t.most_similar(positive=['инвестор', 'рынок'], topn=1)\n",
    "\n",
    "y_train = train['pnl_sign']\n",
    "X_train = train_vectors_gt100\n",
    "\n",
    "y_val = val['pnl_sign']\n",
    "X_val = val_vectors_gt100\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "print(model.score(X_val, y_val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
