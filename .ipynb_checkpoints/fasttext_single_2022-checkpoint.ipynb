{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8495324f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0da7883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f6ee630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm>=4.9.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (4.63.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install \"tqdm>=4.9.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "386b2622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install fasttext==0.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fcbc498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/facebookresearch/fastText.git\n",
    "# !cd fastText\n",
    "# !pip install fastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42d28100",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ccf1c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import fasttext.util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e0f7322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function download_model in module fasttext.util.util:\n",
      "\n",
      "download_model(lang_id, if_exists='strict', dimension=None)\n",
      "    Download pre-trained common-crawl vectors from fastText's website\n",
      "    https://fasttext.cc/docs/en/crawl-vectors.html\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(fasttext.util.download_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34cdbfdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "# fasttext.util.download_model('ru', if_exists='ignore')\n",
    "ft = fasttext.load_model('cc.ru.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5b2382c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.06434693, -0.01527086, -0.06963537, -0.03582602,  0.01471584,\n",
       "       -0.03503159,  0.02701715,  0.04161827, -0.00033126,  0.00355259,\n",
       "        0.06979205,  0.06205348,  0.05154078,  0.03831509, -0.02394784,\n",
       "       -0.03954181, -0.00189653, -0.11174394, -0.0407712 ,  0.09289949,\n",
       "       -0.07412342, -0.05209147,  0.02017231,  0.04837443,  0.02212641,\n",
       "        0.00856511, -0.03055364,  0.04733564,  0.04380886,  0.03856769,\n",
       "        0.03442968,  0.05576854,  0.01513439,  0.14055566,  0.03365337,\n",
       "       -0.02920472, -0.10305687, -0.09332671,  0.03085899, -0.11067575,\n",
       "       -0.08992791,  0.05850704, -0.017424  ,  0.00120653, -0.07153153,\n",
       "        0.10312843, -0.08066262, -0.00642456,  0.04408539, -0.05728461,\n",
       "       -0.0179531 ,  0.03936698,  0.04778077, -0.04907751, -0.00909553,\n",
       "        0.05588715, -0.00236535,  0.04878682, -0.01769035,  0.03295048,\n",
       "        0.00906604,  0.08772802,  0.02970458, -0.04903899, -0.03025401,\n",
       "       -0.04151824,  0.04931813, -0.02804473,  0.05716789,  0.03559401,\n",
       "       -0.12191223,  0.02087349, -0.05121018, -0.0584691 , -0.04781278,\n",
       "       -0.06298476, -0.00432743, -0.03785646, -0.08833752, -0.0375172 ,\n",
       "        0.04602968,  0.02096615,  0.00321184, -0.00927999, -0.00288017,\n",
       "        0.04345381, -0.0330169 ,  0.00840916, -0.05537616, -0.02134524,\n",
       "       -0.03705332,  0.06453154, -0.01733523, -0.01977487, -0.02836509,\n",
       "        0.01901042,  0.04043126, -0.07048826, -0.09381784, -0.02532577,\n",
       "       -0.02679786,  0.01097633, -0.01681483, -0.08134623,  0.00429079,\n",
       "       -0.07213577, -0.03950587,  0.07274695, -0.00337509,  0.05469057,\n",
       "       -0.01510826, -0.05297935,  0.04232059, -0.04494021, -0.01873806,\n",
       "        0.02970697, -0.02128338, -0.07461107,  0.04457341,  0.02913763,\n",
       "       -0.05406609,  0.06825955, -0.0423348 , -0.01933457,  0.00638132,\n",
       "        0.00075826,  0.10154837, -0.06699109, -0.01374834,  0.10683898,\n",
       "        0.06719182,  0.00299954,  0.03092229, -0.01919586,  0.02315286,\n",
       "        0.02552165,  0.0297376 ,  0.0476847 , -0.06794806,  0.01934321,\n",
       "        0.07793375,  0.04631811,  0.07487484, -0.06923444, -0.09797966,\n",
       "       -0.02230856,  0.04383751,  0.05814477,  0.09182699,  0.0407513 ,\n",
       "        0.06562199,  0.06420117, -0.12618978, -0.00895569, -0.03637737,\n",
       "        0.0323772 ,  0.05442533,  0.02233687,  0.0607053 , -0.03511162,\n",
       "       -0.02011008, -0.04657565, -0.1363746 , -0.09366813, -0.01257268,\n",
       "       -0.0822741 ,  0.04026463,  0.08941573,  0.05416025, -0.00148568,\n",
       "        0.02470817, -0.01521165,  0.06688396,  0.01970377, -0.067048  ,\n",
       "        0.05173868, -0.06437217,  0.02638604,  0.02355881, -0.03286408,\n",
       "       -0.01542088,  0.0226214 ,  0.01009578, -0.06503511,  0.05164307,\n",
       "        0.08621447, -0.00291589,  0.0201317 ,  0.05789564,  0.04330945,\n",
       "       -0.01468945,  0.00915974,  0.02692279,  0.07124459, -0.05370982,\n",
       "        0.04218086,  0.00314814,  0.00356758, -0.02068229,  0.0604349 ,\n",
       "       -0.08158811,  0.04939371,  0.0430281 , -0.03372736, -0.0558867 ,\n",
       "        0.00376545, -0.037087  ,  0.05940549,  0.02495521, -0.00334628,\n",
       "        0.01005986, -0.02053031,  0.01179219,  0.07010209, -0.10397089,\n",
       "        0.07733957,  0.06056703,  0.01003617, -0.15787463, -0.01765688,\n",
       "        0.00765593, -0.01905038,  0.01327723,  0.01084452, -0.05930451,\n",
       "       -0.07062402, -0.08540855,  0.01374613, -0.03077546, -0.04025275,\n",
       "        0.00268231, -0.06844234,  0.05945092,  0.02234607,  0.14669767,\n",
       "        0.03161074, -0.03626112,  0.13065669, -0.02795461,  0.00260858,\n",
       "        0.02103085, -0.01555263,  0.02790887, -0.02395738, -0.10259839,\n",
       "       -0.01162906,  0.07939966,  0.01229805,  0.0472665 ,  0.00792722,\n",
       "        0.07495239, -0.04777352,  0.02290227, -0.02349729, -0.01035224,\n",
       "       -0.04128299, -0.05037197, -0.02936148, -0.03995599, -0.09582971,\n",
       "        0.00652219, -0.00309965,  0.09921778, -0.07993053, -0.07525942,\n",
       "       -0.03148453, -0.03216294,  0.00986726, -0.03059178, -0.01402058,\n",
       "        0.02519003, -0.04734642,  0.12286284, -0.09953459,  0.01202545,\n",
       "        0.03934894,  0.07455902,  0.02744922,  0.03999532, -0.05147249,\n",
       "       -0.0055727 ,  0.07064351,  0.07526451,  0.00223117,  0.01765039,\n",
       "       -0.02785274,  0.0251571 ,  0.05081079,  0.06183068, -0.03087618,\n",
       "       -0.00268458, -0.02822061, -0.05344585, -0.05139395,  0.00151552,\n",
       "       -0.01931686,  0.00034288, -0.01423903,  0.00377267, -0.0598783 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft['привет']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2fecc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "def get_tweet_embedding(lemmas, model, embedding_size=300):\n",
    "    \n",
    "    res = np.zeros(embedding_size)\n",
    "    cnt = 0\n",
    "    for word in lemmas.split():\n",
    "        if word in model:\n",
    "            res += np.array(model[word])\n",
    "            cnt += 1\n",
    "    if cnt:\n",
    "        res = res / cnt\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17ef304c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.84749218e-02,  1.14055865e-02, -1.54750008e-02,  6.10717852e-03,\n",
       "       -5.42343501e-03,  2.83443742e-03,  2.40256451e-03,  1.29073053e-02,\n",
       "        3.05031866e-02, -1.99234379e-02,  6.13203850e-02,  4.42768331e-02,\n",
       "        2.71531800e-02, -1.02064133e-02,  9.22483567e-04,  2.50384058e-02,\n",
       "       -1.25383004e-02, -4.89095808e-02, -3.07890818e-02,  1.01918663e-01,\n",
       "       -2.85800546e-02, -1.05811988e-01, -1.28629373e-02,  2.95597422e-02,\n",
       "        2.13206490e-03,  1.26906892e-02, -2.97227059e-02,  2.77029723e-02,\n",
       "       -1.21254625e-02, -4.76178443e-02, -6.68591424e-03,  3.05985650e-02,\n",
       "        3.59081652e-02,  1.02970391e-01,  3.62780495e-02, -5.56655712e-02,\n",
       "       -1.11200343e-01, -1.16946280e-01,  4.69890856e-02, -5.79430675e-02,\n",
       "       -4.56299540e-03, -2.32621958e-03, -2.30524363e-03,  1.96370891e-02,\n",
       "       -1.68996924e-02,  4.77626729e-02, -7.71877861e-02,  2.95996453e-02,\n",
       "        3.40769021e-02, -3.43663241e-02,  5.55797149e-02,  1.05126291e-02,\n",
       "        9.77615127e-03,  3.99762448e-02, -2.15159254e-02,  1.93620831e-02,\n",
       "       -1.34500194e-02,  2.17238814e-03,  1.39238648e-02, -1.24853794e-02,\n",
       "       -5.08000248e-02,  6.46954067e-02,  3.07824350e-02, -4.07074159e-02,\n",
       "       -1.99913508e-02,  1.96010725e-02, -1.52623175e-02, -1.58499312e-02,\n",
       "       -4.72955415e-02,  4.77682170e-03, -5.21384678e-02,  3.73177927e-02,\n",
       "       -3.41336881e-02,  5.90577209e-03,  6.41630217e-03, -2.70765051e-02,\n",
       "       -5.57585534e-02, -9.87857254e-03, -1.75393135e-02, -3.92743144e-02,\n",
       "        2.59191706e-02,  3.97690022e-02,  5.75262937e-03, -4.60929359e-02,\n",
       "        3.89499759e-03,  3.79725420e-02, -6.47511277e-02,  5.06016524e-02,\n",
       "        1.29573480e-02,  9.16208606e-04, -1.10791333e-01, -1.59734851e-02,\n",
       "        4.38956684e-03, -5.00591882e-02, -1.91273557e-02, -1.31234950e-02,\n",
       "        4.23284452e-02, -1.32317897e-02, -9.03501306e-02,  4.14077961e-02,\n",
       "       -9.01901850e-03,  7.61899361e-02, -1.04387742e-02, -1.22785277e-02,\n",
       "       -1.63511078e-02, -3.89259742e-02,  4.25888405e-02,  2.55549918e-02,\n",
       "        7.98778998e-03,  9.49782273e-03, -3.10476529e-02, -1.09063331e-02,\n",
       "        8.43151626e-02, -3.58087434e-02, -1.24211812e-02, -1.87083688e-02,\n",
       "       -5.45124998e-02, -8.86561619e-02,  3.79863534e-02, -3.31068560e-02,\n",
       "       -4.89519509e-02,  7.88756466e-02, -5.50974393e-02,  1.60968699e-02,\n",
       "        1.01266545e-03,  2.75706507e-02,  5.04551297e-02, -1.64009025e-02,\n",
       "        1.19224258e-02,  6.78465860e-02,  2.69761501e-02,  1.12457192e-02,\n",
       "        1.86175751e-02, -1.40191410e-02, -1.72757215e-02,  5.70655339e-02,\n",
       "        5.77574829e-04,  5.24380505e-02, -5.08161918e-03,  3.53807122e-02,\n",
       "       -1.40902330e-02,  2.29508245e-02,  6.68836981e-02, -2.08306434e-02,\n",
       "       -5.37150722e-02, -9.75077925e-03, -4.46044868e-02,  3.50373158e-02,\n",
       "        5.37473205e-02,  2.23793560e-02,  3.78368636e-02,  1.91988172e-02,\n",
       "       -9.24861385e-02,  1.07819675e-02, -1.21989548e-02,  3.31418150e-02,\n",
       "        2.23143799e-02,  1.49282608e-02,  1.40205264e-02, -2.72367133e-02,\n",
       "       -2.69449629e-02,  2.49509839e-03, -5.19380664e-02, -6.78124642e-02,\n",
       "       -6.38052975e-02,  1.68222875e-02, -7.74696337e-02,  8.44685482e-02,\n",
       "        4.74610087e-02, -2.62326931e-03, -9.03137206e-03, -6.54254213e-02,\n",
       "        4.35586069e-02,  3.72354283e-02, -3.41950073e-02, -1.16772181e-02,\n",
       "       -3.14856721e-02,  6.10582798e-02,  4.79020393e-02, -4.82062241e-02,\n",
       "       -1.99088580e-02,  3.10592290e-02, -4.53291640e-02, -6.53951182e-02,\n",
       "        6.01363019e-04,  3.27027021e-02,  1.47671076e-02, -3.70643544e-02,\n",
       "       -2.04997172e-02,  5.32348915e-02, -4.50411410e-02,  1.28656339e-02,\n",
       "        3.19320844e-02,  1.45057300e-02, -1.72404865e-02,  3.71150244e-02,\n",
       "       -2.15479551e-02, -1.15545052e-02, -1.25362631e-02,  2.60218652e-02,\n",
       "       -4.58280314e-02,  7.53374584e-03,  7.35389721e-03, -1.82377142e-02,\n",
       "       -9.48007656e-02, -2.56354164e-02,  5.40375593e-04,  4.41152531e-02,\n",
       "       -7.10717402e-04,  4.32135077e-03,  2.78144046e-02, -4.11721691e-03,\n",
       "       -8.98516446e-04,  5.58417288e-02, -3.63211357e-02, -6.08586776e-03,\n",
       "        3.32013650e-02,  3.86003682e-02, -4.08708490e-02,  1.05813891e-02,\n",
       "        1.79253643e-02,  3.97888436e-02,  3.22979216e-02,  1.34046650e-02,\n",
       "       -3.69763831e-02,  3.80074000e-02, -3.84184653e-02, -1.46874161e-02,\n",
       "        1.31309093e-02, -1.08475601e-02,  1.34081137e-02, -1.17426082e-02,\n",
       "        1.28444180e-01, -2.20539912e-02,  1.15903737e-01,  5.90186799e-03,\n",
       "        1.24620628e-02,  7.31505433e-02, -7.99386785e-03,  3.64330948e-02,\n",
       "        3.45786135e-02,  3.46622248e-02,  2.36364873e-03, -7.05305371e-02,\n",
       "       -5.42092409e-02, -8.53608083e-03, -5.25069842e-03, -6.02257324e-02,\n",
       "        2.03989604e-02, -1.16195149e-02,  9.21856882e-02, -6.69648121e-02,\n",
       "        7.29324436e-03,  2.93620190e-02,  6.43402617e-02,  1.59824028e-02,\n",
       "        2.18187862e-02, -1.51119323e-03, -8.90493509e-03, -1.16167957e-01,\n",
       "        3.43116624e-02, -2.22888631e-02,  6.75987527e-02,  2.60055298e-02,\n",
       "       -2.21864720e-02, -2.88950545e-02,  5.87026319e-02,  5.31624537e-05,\n",
       "        4.37127347e-02, -3.44873604e-02,  1.45444367e-02,  4.66503901e-04,\n",
       "        1.16275493e-01, -7.23772724e-02,  3.08836906e-02,  3.09007196e-03,\n",
       "        1.63774043e-02,  1.35315594e-02,  3.91931003e-02, -3.61349685e-02,\n",
       "        8.19852925e-04,  2.57454347e-04,  6.94709985e-03,  1.86907215e-02,\n",
       "       -3.81673360e-03,  1.24494154e-02, -2.23289109e-02, -3.35482652e-02,\n",
       "       -4.22900976e-02,  2.49405606e-02,  4.42143134e-03,  5.85364044e-03,\n",
       "        3.19063303e-03, -4.88638142e-02, -1.23414861e-02, -1.29953995e-02,\n",
       "        1.96110924e-02,  1.76430468e-02,  9.77955939e-03, -1.76622996e-02])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 'привет всем слушателям курса'\n",
    "get_tweet_embedding(x, model=ft, embedding_size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d77a274",
   "metadata": {},
   "outputs": [],
   "source": [
    "BACKTEST_CCY = 'USDRUB'\n",
    "BACKTEST_DAYS = 5\n",
    "YEAR = 2022\n",
    "\n",
    "PATH_TEXTS = 'data/telegram'\n",
    "PATH_OPT_PNL = 'data/pnl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9b3272b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cbonds.csv',\n",
       " 'themovchans.csv',\n",
       " 'headlines_QUANTS.csv',\n",
       " 'War_Wealth_Wisdom.csv',\n",
       " 'mmi.csv',\n",
       " 'vts.csv',\n",
       " 'signal.csv',\n",
       " '.gitignore',\n",
       " 'rshb_invest.csv',\n",
       " 'Alfa_Wealth.csv',\n",
       " 'sky_bond.csv',\n",
       " 'bitkogan.csv']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all available data_sources\n",
    "sources = os.listdir(PATH_TEXTS)\n",
    "sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c00cad9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_start</th>\n",
       "      <th>pnl</th>\n",
       "      <th>pnl_sign</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-03-02</td>\n",
       "      <td>4.562674e+06</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-03-03</td>\n",
       "      <td>-4.663725e+06</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-03-04</td>\n",
       "      <td>-4.845276e+05</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-03-09</td>\n",
       "      <td>1.268498e+06</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-03-10</td>\n",
       "      <td>-2.874466e+06</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>2022-09-28</td>\n",
       "      <td>-3.886725e+05</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>2022-09-29</td>\n",
       "      <td>8.040320e+05</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>2022-09-30</td>\n",
       "      <td>1.544311e+06</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>2022-10-03</td>\n",
       "      <td>1.868107e+06</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>2022-10-04</td>\n",
       "      <td>2.480969e+06</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>143 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     date_start           pnl  pnl_sign\n",
       "0    2022-03-02  4.562674e+06         1\n",
       "1    2022-03-03 -4.663725e+06         0\n",
       "2    2022-03-04 -4.845276e+05         0\n",
       "3    2022-03-09  1.268498e+06         1\n",
       "4    2022-03-10 -2.874466e+06         0\n",
       "..          ...           ...       ...\n",
       "138  2022-09-28 -3.886725e+05         0\n",
       "139  2022-09-29  8.040320e+05         1\n",
       "140  2022-09-30  1.544311e+06         1\n",
       "141  2022-10-03  1.868107e+06         1\n",
       "142  2022-10-04  2.480969e+06         1\n",
       "\n",
       "[143 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create target variable dataframe\n",
    "pnl = pd.read_csv(f'{PATH_OPT_PNL}/Backtest_{BACKTEST_CCY}_{BACKTEST_DAYS}_days_{YEAR}.txt')\n",
    "pnl['date_start'] = pd.to_datetime(pnl['date_start']).dt.strftime('%Y-%m-%d')\n",
    "pnl['pnl_sign'] = pnl['pnl'].apply(lambda x: 1 if x >= 0 else 0)\n",
    "pnl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c052489b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32167832167832167"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get balance of the sample\n",
    "pnl['pnl_sign'].sum() / pnl.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f953b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "from typing import List, Tuple, Union\n",
    "\n",
    "\n",
    "def _binary_search_by_date(array: List[Tuple[dt.datetime, float]], date_x: dt.datetime) -> Union[int, None]:\n",
    "    \"\"\"\n",
    "    Searches for the index of date_x in the array via binary search.\n",
    "\n",
    "        Parameters:\n",
    "            array (list) : A sorted array of (date, float_value) tuples\n",
    "            date_x (datetime.datetime) : Date to search for\n",
    "\n",
    "        Returns:\n",
    "            index_x (int): Index of the searched date in the array.\n",
    "    \"\"\"\n",
    "    left = 0\n",
    "    right = len(array) - 1\n",
    "\n",
    "    while left <= right:\n",
    "        mid = left + (right - left) // 2\n",
    "\n",
    "        if array[mid - 1][0] <= date_x <= array[mid][0]:\n",
    "            return mid\n",
    "        elif date_x > array[mid - 1][0] and date_x > array[mid][0]:\n",
    "            left = mid + 1\n",
    "        else:\n",
    "            right = mid - 1\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def binary_search_time_series(time_series: List[Tuple[dt.datetime, float]], date_start: dt.datetime,\n",
    "                              date_end: dt.datetime) -> Union[List[Tuple[dt.datetime, float]], None]:\n",
    "    \"\"\"\n",
    "    Searches for the part of the time series that is contained inside [date_start; date_end] period via binary search.\n",
    "\n",
    "        Parameters:\n",
    "            time_series (list) : A sorted array of (date, float_value) tuples\n",
    "            date_start (datetime.datetime) : Starting date of the searched period\n",
    "            date_end (datetime.datetime) : Ending date of the searched period\n",
    "\n",
    "        Returns:\n",
    "            time_series_data (list): Part of the time series that is contained inside [date_start; date_end] period.\n",
    "    \"\"\"\n",
    "\n",
    "    if date_start <= date_end:\n",
    "        left_index = _binary_search_by_date(time_series, date_start)\n",
    "        \n",
    "        if left_index is None:\n",
    "            return None\n",
    "        \n",
    "        right_index = _binary_search_by_date(time_series[left_index:], date_end)\n",
    "        \n",
    "        if right_index is None:\n",
    "            return None\n",
    "        \n",
    "        right_index += left_index\n",
    "    else:\n",
    "        left_index = _binary_search_by_date(time_series, date_end)\n",
    "        \n",
    "        if left_index is None:\n",
    "            return None\n",
    "        \n",
    "        right_index = _binary_search_by_date(time_series[left_index:], date_start)\n",
    "        \n",
    "        if right_index is None:\n",
    "            return None\n",
    "        \n",
    "        right_index += left_index\n",
    "    \n",
    "    return time_series[left_index:right_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "19fac87b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(datetime.datetime(2022, 3, 2, 0, 0), 1),\n",
       " (datetime.datetime(2022, 3, 3, 0, 0), 0),\n",
       " (datetime.datetime(2022, 3, 4, 0, 0), 0),\n",
       " (datetime.datetime(2022, 3, 9, 0, 0), 1),\n",
       " (datetime.datetime(2022, 3, 10, 0, 0), 0),\n",
       " (datetime.datetime(2022, 3, 11, 0, 0), 0),\n",
       " (datetime.datetime(2022, 3, 14, 0, 0), 0),\n",
       " (datetime.datetime(2022, 3, 15, 0, 0), 0),\n",
       " (datetime.datetime(2022, 3, 16, 0, 0), 0),\n",
       " (datetime.datetime(2022, 3, 17, 0, 0), 0),\n",
       " (datetime.datetime(2022, 3, 18, 0, 0), 1),\n",
       " (datetime.datetime(2022, 3, 21, 0, 0), 0),\n",
       " (datetime.datetime(2022, 3, 22, 0, 0), 0),\n",
       " (datetime.datetime(2022, 3, 23, 0, 0), 0),\n",
       " (datetime.datetime(2022, 3, 24, 0, 0), 0),\n",
       " (datetime.datetime(2022, 3, 25, 0, 0), 0),\n",
       " (datetime.datetime(2022, 3, 28, 0, 0), 0),\n",
       " (datetime.datetime(2022, 3, 29, 0, 0), 0),\n",
       " (datetime.datetime(2022, 3, 30, 0, 0), 0),\n",
       " (datetime.datetime(2022, 3, 31, 0, 0), 0),\n",
       " (datetime.datetime(2022, 4, 1, 0, 0), 1),\n",
       " (datetime.datetime(2022, 4, 4, 0, 0), 0),\n",
       " (datetime.datetime(2022, 4, 5, 0, 0), 0),\n",
       " (datetime.datetime(2022, 4, 6, 0, 0), 0),\n",
       " (datetime.datetime(2022, 4, 7, 0, 0), 1),\n",
       " (datetime.datetime(2022, 4, 8, 0, 0), 1),\n",
       " (datetime.datetime(2022, 4, 11, 0, 0), 1),\n",
       " (datetime.datetime(2022, 4, 12, 0, 0), 0),\n",
       " (datetime.datetime(2022, 4, 13, 0, 0), 0),\n",
       " (datetime.datetime(2022, 4, 14, 0, 0), 0),\n",
       " (datetime.datetime(2022, 4, 15, 0, 0), 0),\n",
       " (datetime.datetime(2022, 4, 18, 0, 0), 0),\n",
       " (datetime.datetime(2022, 4, 19, 0, 0), 0),\n",
       " (datetime.datetime(2022, 4, 20, 0, 0), 0),\n",
       " (datetime.datetime(2022, 4, 21, 0, 0), 0),\n",
       " (datetime.datetime(2022, 4, 22, 0, 0), 0),\n",
       " (datetime.datetime(2022, 4, 25, 0, 0), 0),\n",
       " (datetime.datetime(2022, 4, 26, 0, 0), 0),\n",
       " (datetime.datetime(2022, 4, 27, 0, 0), 0),\n",
       " (datetime.datetime(2022, 4, 28, 0, 0), 0),\n",
       " (datetime.datetime(2022, 4, 29, 0, 0), 0),\n",
       " (datetime.datetime(2022, 5, 4, 0, 0), 0),\n",
       " (datetime.datetime(2022, 5, 5, 0, 0), 1),\n",
       " (datetime.datetime(2022, 5, 6, 0, 0), 1),\n",
       " (datetime.datetime(2022, 5, 11, 0, 0), 0),\n",
       " (datetime.datetime(2022, 5, 12, 0, 0), 0),\n",
       " (datetime.datetime(2022, 5, 13, 0, 0), 1),\n",
       " (datetime.datetime(2022, 5, 16, 0, 0), 0),\n",
       " (datetime.datetime(2022, 5, 17, 0, 0), 0),\n",
       " (datetime.datetime(2022, 5, 18, 0, 0), 0),\n",
       " (datetime.datetime(2022, 5, 19, 0, 0), 0),\n",
       " (datetime.datetime(2022, 5, 20, 0, 0), 0),\n",
       " (datetime.datetime(2022, 5, 23, 0, 0), 1),\n",
       " (datetime.datetime(2022, 5, 24, 0, 0), 1),\n",
       " (datetime.datetime(2022, 5, 25, 0, 0), 1),\n",
       " (datetime.datetime(2022, 5, 26, 0, 0), 1),\n",
       " (datetime.datetime(2022, 5, 27, 0, 0), 0),\n",
       " (datetime.datetime(2022, 6, 6, 0, 0), 0),\n",
       " (datetime.datetime(2022, 6, 7, 0, 0), 0),\n",
       " (datetime.datetime(2022, 6, 8, 0, 0), 0),\n",
       " (datetime.datetime(2022, 6, 9, 0, 0), 0),\n",
       " (datetime.datetime(2022, 6, 10, 0, 0), 0),\n",
       " (datetime.datetime(2022, 6, 14, 0, 0), 0),\n",
       " (datetime.datetime(2022, 6, 15, 0, 0), 0),\n",
       " (datetime.datetime(2022, 6, 16, 0, 0), 0),\n",
       " (datetime.datetime(2022, 6, 17, 0, 0), 0),\n",
       " (datetime.datetime(2022, 6, 20, 0, 0), 0),\n",
       " (datetime.datetime(2022, 6, 21, 0, 0), 0),\n",
       " (datetime.datetime(2022, 6, 22, 0, 0), 0),\n",
       " (datetime.datetime(2022, 6, 23, 0, 0), 0),\n",
       " (datetime.datetime(2022, 6, 24, 0, 0), 0),\n",
       " (datetime.datetime(2022, 6, 27, 0, 0), 0),\n",
       " (datetime.datetime(2022, 6, 28, 0, 0), 1),\n",
       " (datetime.datetime(2022, 6, 29, 0, 0), 1),\n",
       " (datetime.datetime(2022, 6, 30, 0, 0), 1),\n",
       " (datetime.datetime(2022, 7, 1, 0, 0), 1),\n",
       " (datetime.datetime(2022, 7, 4, 0, 0), 1),\n",
       " (datetime.datetime(2022, 7, 5, 0, 0), 1),\n",
       " (datetime.datetime(2022, 7, 6, 0, 0), 0),\n",
       " (datetime.datetime(2022, 7, 7, 0, 0), 0),\n",
       " (datetime.datetime(2022, 7, 8, 0, 0), 0),\n",
       " (datetime.datetime(2022, 7, 11, 0, 0), 0),\n",
       " (datetime.datetime(2022, 7, 12, 0, 0), 0),\n",
       " (datetime.datetime(2022, 7, 13, 0, 0), 0),\n",
       " (datetime.datetime(2022, 7, 14, 0, 0), 0),\n",
       " (datetime.datetime(2022, 7, 15, 0, 0), 0),\n",
       " (datetime.datetime(2022, 7, 18, 0, 0), 0),\n",
       " (datetime.datetime(2022, 7, 19, 0, 0), 0),\n",
       " (datetime.datetime(2022, 7, 20, 0, 0), 1),\n",
       " (datetime.datetime(2022, 7, 21, 0, 0), 1),\n",
       " (datetime.datetime(2022, 7, 22, 0, 0), 1),\n",
       " (datetime.datetime(2022, 7, 25, 0, 0), 1),\n",
       " (datetime.datetime(2022, 7, 26, 0, 0), 1),\n",
       " (datetime.datetime(2022, 7, 27, 0, 0), 1),\n",
       " (datetime.datetime(2022, 7, 28, 0, 0), 0),\n",
       " (datetime.datetime(2022, 7, 29, 0, 0), 0),\n",
       " (datetime.datetime(2022, 8, 1, 0, 0), 0),\n",
       " (datetime.datetime(2022, 8, 2, 0, 0), 1),\n",
       " (datetime.datetime(2022, 8, 3, 0, 0), 0),\n",
       " (datetime.datetime(2022, 8, 4, 0, 0), 0),\n",
       " (datetime.datetime(2022, 8, 5, 0, 0), 0),\n",
       " (datetime.datetime(2022, 8, 8, 0, 0), 0),\n",
       " (datetime.datetime(2022, 8, 9, 0, 0), 0),\n",
       " (datetime.datetime(2022, 8, 10, 0, 0), 1),\n",
       " (datetime.datetime(2022, 8, 11, 0, 0), 1),\n",
       " (datetime.datetime(2022, 8, 12, 0, 0), 1),\n",
       " (datetime.datetime(2022, 8, 15, 0, 0), 0),\n",
       " (datetime.datetime(2022, 8, 16, 0, 0), 0),\n",
       " (datetime.datetime(2022, 8, 17, 0, 0), 0),\n",
       " (datetime.datetime(2022, 8, 18, 0, 0), 0),\n",
       " (datetime.datetime(2022, 8, 19, 0, 0), 1),\n",
       " (datetime.datetime(2022, 8, 22, 0, 0), 1),\n",
       " (datetime.datetime(2022, 8, 23, 0, 0), 1),\n",
       " (datetime.datetime(2022, 8, 24, 0, 0), 1),\n",
       " (datetime.datetime(2022, 8, 25, 0, 0), 1),\n",
       " (datetime.datetime(2022, 8, 26, 0, 0), 0),\n",
       " (datetime.datetime(2022, 8, 29, 0, 0), 0),\n",
       " (datetime.datetime(2022, 8, 30, 0, 0), 1),\n",
       " (datetime.datetime(2022, 8, 31, 0, 0), 1),\n",
       " (datetime.datetime(2022, 9, 1, 0, 0), 1),\n",
       " (datetime.datetime(2022, 9, 2, 0, 0), 1),\n",
       " (datetime.datetime(2022, 9, 5, 0, 0), 0),\n",
       " (datetime.datetime(2022, 9, 6, 0, 0), 0),\n",
       " (datetime.datetime(2022, 9, 7, 0, 0), 0),\n",
       " (datetime.datetime(2022, 9, 8, 0, 0), 0),\n",
       " (datetime.datetime(2022, 9, 9, 0, 0), 0),\n",
       " (datetime.datetime(2022, 9, 12, 0, 0), 0),\n",
       " (datetime.datetime(2022, 9, 13, 0, 0), 0),\n",
       " (datetime.datetime(2022, 9, 14, 0, 0), 1),\n",
       " (datetime.datetime(2022, 9, 15, 0, 0), 1),\n",
       " (datetime.datetime(2022, 9, 16, 0, 0), 1),\n",
       " (datetime.datetime(2022, 9, 19, 0, 0), 0),\n",
       " (datetime.datetime(2022, 9, 20, 0, 0), 0),\n",
       " (datetime.datetime(2022, 9, 21, 0, 0), 0),\n",
       " (datetime.datetime(2022, 9, 22, 0, 0), 0),\n",
       " (datetime.datetime(2022, 9, 23, 0, 0), 0),\n",
       " (datetime.datetime(2022, 9, 26, 0, 0), 0),\n",
       " (datetime.datetime(2022, 9, 27, 0, 0), 0),\n",
       " (datetime.datetime(2022, 9, 28, 0, 0), 0),\n",
       " (datetime.datetime(2022, 9, 29, 0, 0), 1),\n",
       " (datetime.datetime(2022, 9, 30, 0, 0), 1),\n",
       " (datetime.datetime(2022, 10, 3, 0, 0), 1),\n",
       " (datetime.datetime(2022, 10, 4, 0, 0), 1)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pnl_sign_ts = [(pd.to_datetime(row['date_start']).to_pydatetime(), row['pnl_sign']) for  _, row in pnl.iterrows()]\n",
    "pnl_sign_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aaf278da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sr/lzvmv9j54ks_jl4x99t250r00000gn/T/ipykernel_26199/371081446.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(source_data)\n",
      "/var/folders/sr/lzvmv9j54ks_jl4x99t250r00000gn/T/ipykernel_26199/371081446.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(source_data)\n",
      "/var/folders/sr/lzvmv9j54ks_jl4x99t250r00000gn/T/ipykernel_26199/371081446.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(source_data)\n",
      "/var/folders/sr/lzvmv9j54ks_jl4x99t250r00000gn/T/ipykernel_26199/371081446.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(source_data)\n",
      "/var/folders/sr/lzvmv9j54ks_jl4x99t250r00000gn/T/ipykernel_26199/371081446.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(source_data)\n",
      "/var/folders/sr/lzvmv9j54ks_jl4x99t250r00000gn/T/ipykernel_26199/371081446.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(source_data)\n",
      "/var/folders/sr/lzvmv9j54ks_jl4x99t250r00000gn/T/ipykernel_26199/371081446.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(source_data)\n",
      "/var/folders/sr/lzvmv9j54ks_jl4x99t250r00000gn/T/ipykernel_26199/371081446.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(source_data)\n",
      "/var/folders/sr/lzvmv9j54ks_jl4x99t250r00000gn/T/ipykernel_26199/371081446.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(source_data)\n",
      "/var/folders/sr/lzvmv9j54ks_jl4x99t250r00000gn/T/ipykernel_26199/371081446.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(source_data)\n",
      "/var/folders/sr/lzvmv9j54ks_jl4x99t250r00000gn/T/ipykernel_26199/371081446.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(source_data)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>2017-07-28T10:56:14</td>\n",
       "      <td>Cbonds.ru  запустило канал в Telegram. Планиру...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>2017-08-03T15:28:17</td>\n",
       "      <td>Облигационный бюллетень Cbonds – все данные о ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>2017-08-03T15:41:02</td>\n",
       "      <td>Cbonds prepared monthly report: CBONDS GLOBAL ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>2017-08-03T18:11:32</td>\n",
       "      <td>Дайджест Cbonds от  3 августа:«Группа Компаний...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>2017-08-04T12:42:41</td>\n",
       "      <td>Совкомбанк открыл книгу заявок на вторичное ра...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                 date                                               text\n",
       "0   9  2017-07-28T10:56:14  Cbonds.ru  запустило канал в Telegram. Планиру...\n",
       "1  10  2017-08-03T15:28:17  Облигационный бюллетень Cbonds – все данные о ...\n",
       "2  11  2017-08-03T15:41:02  Cbonds prepared monthly report: CBONDS GLOBAL ...\n",
       "3  12  2017-08-03T18:11:32  Дайджест Cbonds от  3 августа:«Группа Компаний...\n",
       "4  13  2017-08-04T12:42:41  Совкомбанк открыл книгу заявок на вторичное ра..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate dataframes\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for s in sources:\n",
    "    if s != '.gitignore':\n",
    "        source_data = pd.read_csv(f'{PATH_TEXTS}/{s}')\n",
    "        df = df.append(source_data)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "53d077c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial_date = pnl_sign_ts[0][0]\n",
    "\n",
    "# for i, row in df.iterrows():\n",
    "#     date_x = pd.to_datetime(row['date']).to_pydatetime()\n",
    "#     pnl_sign_key = _binary_search_by_date(pnl_sign_ts, date_x)\n",
    "    \n",
    "#     if pnl_sign_key is not None and date_x >= initial_date:\n",
    "#         df.loc[df.index[i], 'pnl_sign'] = pnl_sign_ts[_binary_search_by_date(pnl_sign_ts, date_x)][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a3d7b52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_date = pnl_sign_ts[0][0]\n",
    "\n",
    "def get_pnl_sign(row):\n",
    "    date_x = pd.to_datetime(row['date']).to_pydatetime()\n",
    "    pnl_sign_key = _binary_search_by_date(pnl_sign_ts, date_x)\n",
    "    \n",
    "    if pnl_sign_key is not None and date_x >= initial_date:\n",
    "        return pnl_sign_ts[_binary_search_by_date(pnl_sign_ts, date_x)][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "75961477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>pnl_sign</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>2017-07-28T10:56:14</td>\n",
       "      <td>Cbonds.ru  запустило канал в Telegram. Планиру...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>2017-08-03T15:28:17</td>\n",
       "      <td>Облигационный бюллетень Cbonds – все данные о ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>2017-08-03T15:41:02</td>\n",
       "      <td>Cbonds prepared monthly report: CBONDS GLOBAL ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>2017-08-03T18:11:32</td>\n",
       "      <td>Дайджест Cbonds от  3 августа:«Группа Компаний...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>2017-08-04T12:42:41</td>\n",
       "      <td>Совкомбанк открыл книгу заявок на вторичное ра...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                 date                                               text  \\\n",
       "0   9  2017-07-28T10:56:14  Cbonds.ru  запустило канал в Telegram. Планиру...   \n",
       "1  10  2017-08-03T15:28:17  Облигационный бюллетень Cbonds – все данные о ...   \n",
       "2  11  2017-08-03T15:41:02  Cbonds prepared monthly report: CBONDS GLOBAL ...   \n",
       "3  12  2017-08-03T18:11:32  Дайджест Cbonds от  3 августа:«Группа Компаний...   \n",
       "4  13  2017-08-04T12:42:41  Совкомбанк открыл книгу заявок на вторичное ра...   \n",
       "\n",
       "   pnl_sign  \n",
       "0       NaN  \n",
       "1       NaN  \n",
       "2       NaN  \n",
       "3       NaN  \n",
       "4       NaN  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['pnl_sign'] = df.apply(lambda row: get_pnl_sign(row), axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a02b786d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>pnl_sign</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10813</td>\n",
       "      <td>2022-03-02T09:38:25</td>\n",
       "      <td>УТРЕННИЙ ДАЙДЖЕСТ  🏛❗️Банк России принял решен...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10814</td>\n",
       "      <td>2022-03-02T10:49:57</td>\n",
       "      <td>#НовостиКомпаний  ⚡️ Российские компании: осно...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10815</td>\n",
       "      <td>2022-03-02T11:06:47</td>\n",
       "      <td>⚡️ Важное на рынках: ⛔️Международная ассоциаци...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10816</td>\n",
       "      <td>2022-03-02T11:23:57</td>\n",
       "      <td>📝 НАУФОР просит рассмотреть вопрос дополнитель...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10817</td>\n",
       "      <td>2022-03-02T12:23:55</td>\n",
       "      <td>#ДенежныйРынок 📆 💰События денежного рынка сего...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                 date  \\\n",
       "0  10813  2022-03-02T09:38:25   \n",
       "1  10814  2022-03-02T10:49:57   \n",
       "2  10815  2022-03-02T11:06:47   \n",
       "3  10816  2022-03-02T11:23:57   \n",
       "4  10817  2022-03-02T12:23:55   \n",
       "\n",
       "                                                text  pnl_sign  \n",
       "0  УТРЕННИЙ ДАЙДЖЕСТ  🏛❗️Банк России принял решен...       0.0  \n",
       "1  #НовостиКомпаний  ⚡️ Российские компании: осно...       0.0  \n",
       "2  ⚡️ Важное на рынках: ⛔️Международная ассоциаци...       0.0  \n",
       "3  📝 НАУФОР просит рассмотреть вопрос дополнитель...       0.0  \n",
       "4  #ДенежныйРынок 📆 💰События денежного рынка сего...       0.0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(subset=['pnl_sign'], inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4e7f8728",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['id', 'date', 'pnl_sign'], axis=1)\n",
    "y = df['pnl_sign']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3f3545b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        УТРЕННИЙ ДАЙДЖЕСТ  🏛❗️Банк России принял решен...\n",
       "1        #НовостиКомпаний  ⚡️ Российские компании: осно...\n",
       "2        ⚡️ Важное на рынках: ⛔️Международная ассоциаци...\n",
       "3        📝 НАУФОР просит рассмотреть вопрос дополнитель...\n",
       "4        #ДенежныйРынок 📆 💰События денежного рынка сего...\n",
       "                               ...                        \n",
       "14922    Завершаем начатую в субботу серию  заметок  о ...\n",
       "14923    Санкции на НКЦ = санкции на американские акции...\n",
       "14924    В среду в Вене группа ОПЕК+ встретится, чтобы ...\n",
       "14925    На прошлой неделе в  сервисе по подписке  мы з...\n",
       "14926    Торги на валютном рынке сегодня проходят нетип...\n",
       "Name: text, Length: 14927, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7b6e3c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from functools import lru_cache\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm.gui import tqdm as tqdm_gui\n",
    "\n",
    "m = MorphAnalyzer()\n",
    "regex = re.compile(\"[а-яa-zёЁ]+\")\n",
    "\n",
    "def words_only(text, regex=regex):\n",
    "    try:\n",
    "        return regex.findall(text.lower())\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "@lru_cache(maxsize=128)\n",
    "def lemmatize_word(token, pymorphy=m):\n",
    "    return pymorphy.parse(token)[0].normal_form\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatize_word(w) for w in text]\n",
    "\n",
    "mystopwords = stopwords.words('russian') \n",
    "def remove_stopwords(lemmas, stopwords = mystopwords):\n",
    "    return [w for w in lemmas if not w in stopwords and len(w) > 3]\n",
    "\n",
    "def clean_text(text):\n",
    "    tokens = words_only(text)\n",
    "    lemmas = lemmatize_text(tokens)\n",
    "    \n",
    "    return ' '.join(remove_stopwords(lemmas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "51c88026",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=121)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "09b2f9a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd482ae030cb46819b6f3991395a1c64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11941 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lemmas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9288</th>\n",
       "      <td>#Макро \\n 📊 Опрос: Маск — красавчик? В апреле ...</td>\n",
       "      <td>макро опрос маск красавчик апрель опросить отз...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10636</th>\n",
       "      <td>#Макро \\n🇷🇺 Санкции против России не смогли по...</td>\n",
       "      <td>макро санкция против россия смочь подорвать фи...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4785</th>\n",
       "      <td>#Commodities \\n📊  Товарные   биржи Цены фьючер...</td>\n",
       "      <td>commodities товарный биржа цена фьючерс основн...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8707</th>\n",
       "      <td>#Commodities \\n📊  Товарные   биржи Цены фьючер...</td>\n",
       "      <td>commodities товарный биржа цена фьючерс основн...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2392</th>\n",
       "      <td>‼️ Все карты международных платежных систем Vi...</td>\n",
       "      <td>карта международный платёжный система visa mas...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  \\\n",
       "9288   #Макро \\n 📊 Опрос: Маск — красавчик? В апреле ...   \n",
       "10636  #Макро \\n🇷🇺 Санкции против России не смогли по...   \n",
       "4785   #Commodities \\n📊  Товарные   биржи Цены фьючер...   \n",
       "8707   #Commodities \\n📊  Товарные   биржи Цены фьючер...   \n",
       "2392   ‼️ Все карты международных платежных систем Vi...   \n",
       "\n",
       "                                                  lemmas  \n",
       "9288   макро опрос маск красавчик апрель опросить отз...  \n",
       "10636  макро санкция против россия смочь подорвать фи...  \n",
       "4785   commodities товарный биржа цена фьючерс основн...  \n",
       "8707   commodities товарный биржа цена фьючерс основн...  \n",
       "2392   карта международный платёжный система visa mas...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmas = list(tqdm(map(clean_text, X_train['text']), total=len(X_train)))\n",
    "\n",
    "X_train['lemmas'] = lemmas\n",
    "X_train.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7159b314",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15375699ad784bee92dda85176e31cf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2986 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lemmas_val = list(tqdm(map(clean_text, X_val['text']), total=len(X_val)))\n",
    "\n",
    "X_val['lemmas'] = lemmas_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "84cde13f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11941, 2)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9e76422a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6cc1815d7a84f959367fe1fc3242f09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11941 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train done\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "X_train['embedding'] = X_train['lemmas'].progress_apply(lambda x: get_tweet_embedding(x, model=ft))\n",
    "print('train done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d4b9cb6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c82e46dff9b45fb89bc64c9dc4acfe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2986 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "X_val['embedding'] = X_val['lemmas'].progress_apply(lambda x: get_tweet_embedding(x, model=ft))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7c58229b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embedding = np.array(list(X_train['embedding'].values))\n",
    "val_embedding = np.array(list(X_val['embedding'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3ca0cbbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6993551628841805"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = LogisticRegression(random_state=12)\n",
    "clf.fit(train_embedding, y_train)\n",
    "clf.score(train_embedding, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dfae93e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6999330207635633"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = clf.predict(val_embedding)\n",
    "accuracy_score(pred, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f1ae4906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.695244474212994"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(max_features=4, n_estimators=50)\n",
    "\n",
    "rf.fit(train_embedding, y_train)\n",
    "(y_val == rf.predict(val_embedding)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f27bf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f32cdbf9fca41cb959bb02b9b0dc41f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MetricVisualizer(layout=Layout(align_self='stretch', height='500px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import catboost as cb\n",
    "\n",
    "np.random.seed(12)\n",
    "\n",
    "params = dict(\n",
    "    learning_rate=0.025,\n",
    "    iterations=10000,\n",
    "    reg_lambda=0.0005,\n",
    "    colsample_bylevel=1.,\n",
    "    max_bin=80,\n",
    "    bagging_temperature=2,\n",
    "    use_best_model=True,\n",
    "    verbose=False,\n",
    "    grow_policy='Depthwise',\n",
    "    random_seed=12\n",
    ")\n",
    "model = cb.CatBoostClassifier(\n",
    "    **params,\n",
    ")\n",
    "\n",
    "eval_set = cb.Pool(data=val_embedding, label=y_val)\n",
    "model.fit(train_embedding, y_train, eval_set=eval_set, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f69c134",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(val_embedding)\n",
    "accuracy_score(pred, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de8b7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = list(tqdm(map(clean_text, df['text']), total=len(df)))\n",
    "\n",
    "df['lemmas'] = lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344297db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, val = train_test_split(df, test_size=0.2, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a423224",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_ft.txt', 'w') as f:\n",
    "    for label, lemmas in list(zip(\n",
    "        y_train, X_train['lemmas']\n",
    "    )):\n",
    "        f.write(f\"__label__{int(label)} {lemmas}\\n\")\n",
    "        #print(f\"__label__{int(label)} {lemmas}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25163fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tail train_ft.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996c9c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = fasttext.train_supervised('train_ft.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9371ecc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = classifier.predict(list(X_val['lemmas']))[0]\n",
    "pred = [int(label[0][-1]) for label in pred]\n",
    "\n",
    "accuracy_score(list(y_val), pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1fef11",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_tweets = [tweet.split() for tweet in train['lemmas'].values]\n",
    "\n",
    "%time w2v = word2vec.Word2Vec(tokenized_tweets, workers=4, vector_size=200, min_count=10, window=3, sample=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfed6754",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.wv.most_similar(positive=['рост'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee7fb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweet_embedding(lemmas, model=w2v.wv, embedding_size=200):\n",
    "    \n",
    "    res = np.zeros(embedding_size)\n",
    "    cnt = 0\n",
    "    for word in lemmas.split():\n",
    "        if word in model:\n",
    "            res += np.array(model[word])\n",
    "            cnt += 1\n",
    "    if cnt:\n",
    "        res = res / cnt\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c181c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['w2v_embedding'] = X_train['lemmas'].map(get_tweet_embedding)\n",
    "X_val['w2v_embedding'] = X_val['lemmas'].map(get_tweet_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4694775",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_w2v = list(X_train['w2v_embedding'].values)\n",
    "val_w2v = list(X_val['w2v_embedding'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10b33c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=12, max_iter=500)\n",
    "clf.fit(train_w2v, y_train)\n",
    "\n",
    "pred = clf.predict(val_w2v)\n",
    "accuracy_score(pred, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069e738a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import catboost as cb\n",
    "\n",
    "np.random.seed(12)\n",
    "\n",
    "params = dict(\n",
    "    learning_rate=0.025,\n",
    "    iterations=10000,\n",
    "    reg_lambda=0.0005,\n",
    "    colsample_bylevel=1.,\n",
    "    max_bin=80,\n",
    "    bagging_temperature=2,\n",
    "    use_best_model=True,\n",
    "    verbose=False,\n",
    "    grow_policy='Depthwise',\n",
    "    random_seed=12\n",
    ")\n",
    "model = cb.CatBoostClassifier(\n",
    "    **params,\n",
    ")\n",
    "\n",
    "eval_set = cb.Pool(data=val_w2v, label=y_val)\n",
    "model.fit(train_w2v, y_train, eval_set=eval_set, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2145fc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(val_w2v)\n",
    "accuracy_score(pred, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5e4e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm  \n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim import utils\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n",
    "\n",
    "# import dlnlputils\n",
    "# from dlnlputils.data import tokenize_text_simple_regex, tokenize_corpus, build_vocabulary, \\\n",
    "#     vectorize_texts, SparseFeaturesDataset\n",
    "# from dlnlputils.pipeline import train_eval_loop, predict_with_model, init_random_seed\n",
    "\n",
    "# init_random_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86569712",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "TOKEN_RE = re.compile(r'[\\w\\d]+')\n",
    "\n",
    "\n",
    "def tokenize_text_simple_regex(txt, min_token_size=4):\n",
    "    txt = txt.lower()\n",
    "    all_tokens = TOKEN_RE.findall(txt)\n",
    "    return [token for token in all_tokens if len(token) >= min_token_size]\n",
    "\n",
    "\n",
    "def character_tokenize(txt):\n",
    "    return list(txt)\n",
    "\n",
    "\n",
    "def tokenize_corpus(texts, tokenizer=tokenize_text_simple_regex, **tokenizer_kwargs):\n",
    "    return [tokenizer(text, **tokenizer_kwargs) for text in texts]\n",
    "\n",
    "\n",
    "def add_fake_token(word2id, token=''):\n",
    "    word2id_new = {token: i + 1 for token, i in word2id.items()}\n",
    "    word2id_new[token] = 0\n",
    "    return word2id_new\n",
    "\n",
    "\n",
    "def texts_to_token_ids(tokenized_texts, word2id):\n",
    "    return [[word2id[token] for token in text if token in word2id]\n",
    "            for text in tokenized_texts]\n",
    "\n",
    "\n",
    "def build_vocabulary(tokenized_texts, max_size=1000000, max_doc_freq=0.8, min_count=5, pad_word=None):\n",
    "    word_counts = collections.defaultdict(int)\n",
    "    doc_n = 0\n",
    "\n",
    "    # посчитать количество документов, в которых употребляется каждое слово\n",
    "    # а также общее количество документов\n",
    "    for txt in tokenized_texts:\n",
    "        doc_n += 1\n",
    "        unique_text_tokens = set(txt)\n",
    "        for token in unique_text_tokens:\n",
    "            word_counts[token] += 1\n",
    "\n",
    "    # убрать слишком редкие и слишком частые слова\n",
    "    word_counts = {word: cnt for word, cnt in word_counts.items()\n",
    "                   if cnt >= min_count and cnt / doc_n <= max_doc_freq}\n",
    "\n",
    "    # отсортировать слова по убыванию частоты\n",
    "    sorted_word_counts = sorted(word_counts.items(),\n",
    "                                reverse=True,\n",
    "                                key=lambda pair: pair[1])\n",
    "\n",
    "    # добавим несуществующее слово с индексом 0 для удобства пакетной обработки\n",
    "    if pad_word is not None:\n",
    "        sorted_word_counts = [(pad_word, 0)] + sorted_word_counts\n",
    "\n",
    "    # если у нас по прежнему слишком много слов, оставить только max_size самых частотных\n",
    "    if len(word_counts) > max_size:\n",
    "        sorted_word_counts = sorted_word_counts[:max_size]\n",
    "\n",
    "    # нумеруем слова\n",
    "    word2id = {word: i for i, (word, _) in enumerate(sorted_word_counts)}\n",
    "\n",
    "    # нормируем частоты слов\n",
    "    word2freq = np.array([cnt / doc_n for _, cnt in sorted_word_counts], dtype='float32')\n",
    "\n",
    "    return word2id, word2freq\n",
    "\n",
    "\n",
    "PAD_TOKEN = '__PAD__'\n",
    "NUMERIC_TOKEN = '__NUMBER__'\n",
    "NUMERIC_RE = re.compile(r'^([0-9.,e+\\-]+|[mcxvi]+)$', re.I)\n",
    "\n",
    "\n",
    "def replace_number_nokens(tokenized_texts):\n",
    "    return [[token if not NUMERIC_RE.match(token) else NUMERIC_TOKEN for token in text]\n",
    "            for text in tokenized_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e0c606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "def vectorize_texts(tokenized_texts, word2id, word2freq, mode='tfidf', scale=True):\n",
    "    assert mode in {'tfidf', 'idf', 'tf', 'bin'}\n",
    "\n",
    "    # считаем количество употреблений каждого слова в каждом документе\n",
    "    result = scipy.sparse.dok_matrix((len(tokenized_texts), len(word2id)), dtype='float32')\n",
    "    for text_i, text in enumerate(tokenized_texts):\n",
    "        for token in text:\n",
    "            if token in word2id:\n",
    "                result[text_i, word2id[token]] += 1\n",
    "\n",
    "    # получаем бинарные вектора \"встречается или нет\"\n",
    "    if mode == 'bin':\n",
    "        result = (result > 0).astype('float32')\n",
    "\n",
    "    # получаем вектора относительных частот слова в документе\n",
    "    elif mode == 'tf':\n",
    "        result = result.tocsr()\n",
    "        result = result.multiply(1 / result.sum(1))\n",
    "\n",
    "    # полностью убираем информацию о количестве употреблений слова в данном документе,\n",
    "    # но оставляем информацию о частотности слова в корпусе в целом\n",
    "    elif mode == 'idf':\n",
    "        result = (result > 0).astype('float32').multiply(1 / word2freq)\n",
    "\n",
    "    # учитываем всю информацию, которая у нас есть:\n",
    "    # частоту слова в документе и частоту слова в корпусе\n",
    "    elif mode == 'tfidf':\n",
    "        result = result.tocsr()\n",
    "        result = result.multiply(1 / result.sum(1))  # разделить каждую строку на её длину\n",
    "        result = result.multiply(1 / word2freq)  # разделить каждый столбец на вес слова\n",
    "\n",
    "    if scale:\n",
    "        result = result.tocsc()\n",
    "        result -= result.min()\n",
    "        result /= (result.max() + 1e-6)\n",
    "\n",
    "    return result.tocsr()\n",
    "\n",
    "\n",
    "class SparseFeaturesDataset(Dataset):\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.features.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        cur_features = torch.from_numpy(self.features[idx].toarray()[0]).float()\n",
    "        cur_label = torch.from_numpy(np.asarray(self.targets[idx])).long()\n",
    "        return cur_features, cur_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3793d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokenized = tokenize_corpus(X_train['text'])\n",
    "val_tokenized = tokenize_corpus(X_val['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395e4250",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(' '.join(train_tokenized[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a12687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# строим словарь - vocabulary с помощью функции build_vocabulary\n",
    "# принимает на вход список списков токенезированные\n",
    "# word_doc_freq - содержит относительные частоты всех слов в датасете, он понадобиться \n",
    "# на этапе формирования матрицы признаков\n",
    "\n",
    "MAX_DF = 0.8 #во скольких документах встречаеться слово\n",
    "MIN_COUNT = 5 # сколько раз слово встречаеться в тексте\n",
    "\n",
    "\n",
    "vocabulary, word_doc_freq = build_vocabulary(train_tokenized, max_doc_freq=MAX_DF, min_count=MIN_COUNT)\n",
    "UNIQUE_WORDS_N = len(vocabulary)\n",
    "print('Количество уникальных токенов', UNIQUE_WORDS_N)\n",
    "print(list(vocabulary.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebd3e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(word_doc_freq, bins=20)\n",
    "plt.title('Распределение относительных частот слов')\n",
    "plt.yscale('log');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0faaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "VECTORIZATION_MODE = 'tfidf'\n",
    "# построение матрицы признаков по методу мешка слов\n",
    "# функция vectorize_texts принимает на вход\n",
    "#1. токенизированные список списков\n",
    "#2. словарь\n",
    "#3. вектор частоты токенизированны\n",
    "#4. алгоритм взвешивания токенов по частоте mode - есть 4 алгорима - bin,tf,idf,tfidf\n",
    "#5. флаг чтобы перемаштабировать флаг после взвешивания\n",
    "\n",
    "train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n",
    "\n",
    "print('Размерность матрицы признаков обучающей выборки', train_vectors.shape)\n",
    "print()\n",
    "print('Количество ненулевых элементов в обучающей выборке', train_vectors.nnz)\n",
    "print('Процент заполненности матрицы признаков {:.2f}%'.format(train_vectors.nnz * 100 / (train_vectors.shape[0] * train_vectors.shape[1])))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686d2508",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_t = api.load('glove-twitter-100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa419ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import utils\n",
    "\n",
    "def get_vectors_gt100(row):\n",
    "    '''\n",
    "      word_doc_freq # частоты слов\n",
    "      train_tokenized #сами слова\n",
    "    '''\n",
    "    vecs = [np.zeros(100)]\n",
    "    for word in row:\n",
    "        #print(row)\n",
    "        try: \n",
    "            # если слово есть в нашем очищенном словаре\n",
    "            # умножаем вектор на вес tfidf\n",
    "            v = model_t[word] * word_doc_freq[vocabulary[word]] \n",
    "        except:\n",
    "            v = np.zeros(100)\n",
    "        vecs.append(v)\n",
    "    return np.sum(np.array(vecs),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2e5892",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gt100 = np.array([get_vectors_gt100(i) for i in train_tokenized])\n",
    "val_gt100 = np.array([get_vectors_gt100(i) for i in val_tokenized])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2730d1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_t.most_similar(positive=['инвестор', 'рынок'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd22560",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=12, max_iter=500)\n",
    "clf.fit(train_gt100, y_train)\n",
    "\n",
    "pred = clf.predict(val_gt100)\n",
    "accuracy_score(pred, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c095ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import catboost as cb\n",
    "\n",
    "np.random.seed(12)\n",
    "\n",
    "params = dict(\n",
    "    learning_rate=0.025,\n",
    "    iterations=10000,\n",
    "    reg_lambda=0.0005,\n",
    "    colsample_bylevel=1.,\n",
    "    max_bin=80,\n",
    "    bagging_temperature=2,\n",
    "    use_best_model=True,\n",
    "    verbose=False,\n",
    "    grow_policy='Depthwise',\n",
    "    random_seed=12\n",
    ")\n",
    "model = cb.CatBoostClassifier(\n",
    "    **params,\n",
    ")\n",
    "\n",
    "eval_set = cb.Pool(data=val_gt100, label=y_val)\n",
    "model.fit(train_gt100, y_train, eval_set=eval_set, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81132739",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(val_w2v)\n",
    "accuracy_score(pred, y_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
