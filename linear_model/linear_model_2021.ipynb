{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4743ed93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.6.4\r\n"
     ]
    }
   ],
   "source": [
    "!pyenv global 3.6\n",
    "!eval \"$(pyenv init --path)\"\n",
    "!python3 --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29895fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/Samsung-IT-Academy/stepik-dl-nlp.git && pip install -r stepik-dl-nlp/requirements.txt\n",
    "import sys; sys.path.append('./stepik-dl-nlp')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm  \n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim import utils\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n",
    "\n",
    "# import dlnlputils\n",
    "# from dlnlputils.data import tokenize_text_simple_regex, tokenize_corpus, build_vocabulary, \\\n",
    "#     vectorize_texts, SparseFeaturesDataset\n",
    "# from dlnlputils.pipeline import train_eval_loop, predict_with_model, init_random_seed\n",
    "\n",
    "# init_random_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "129bab82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse\n",
    "import torch\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318333aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#подготовка признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "023efcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "TOKEN_RE = re.compile(r'[\\w\\d]+')\n",
    "\n",
    "\n",
    "def tokenize_text_simple_regex(txt, min_token_size=4):\n",
    "    txt = txt.lower()\n",
    "    all_tokens = TOKEN_RE.findall(txt)\n",
    "    return [token for token in all_tokens if len(token) >= min_token_size]\n",
    "\n",
    "\n",
    "def character_tokenize(txt):\n",
    "    return list(txt)\n",
    "\n",
    "\n",
    "def tokenize_corpus(texts, tokenizer=tokenize_text_simple_regex, **tokenizer_kwargs):\n",
    "    return [tokenizer(text, **tokenizer_kwargs) for text in texts]\n",
    "\n",
    "\n",
    "def add_fake_token(word2id, token=''):\n",
    "    word2id_new = {token: i + 1 for token, i in word2id.items()}\n",
    "    word2id_new[token] = 0\n",
    "    return word2id_new\n",
    "\n",
    "\n",
    "def texts_to_token_ids(tokenized_texts, word2id):\n",
    "    return [[word2id[token] for token in text if token in word2id]\n",
    "            for text in tokenized_texts]\n",
    "\n",
    "\n",
    "def build_vocabulary(tokenized_texts, max_size=1000000, max_doc_freq=0.8, min_count=5, pad_word=None):\n",
    "    word_counts = collections.defaultdict(int)\n",
    "    doc_n = 0\n",
    "\n",
    "    # посчитать количество документов, в которых употребляется каждое слово\n",
    "    # а также общее количество документов\n",
    "    for txt in tokenized_texts:\n",
    "        doc_n += 1\n",
    "        unique_text_tokens = set(txt)\n",
    "        for token in unique_text_tokens:\n",
    "            word_counts[token] += 1\n",
    "\n",
    "    # убрать слишком редкие и слишком частые слова\n",
    "    word_counts = {word: cnt for word, cnt in word_counts.items()\n",
    "                   if cnt >= min_count and cnt / doc_n <= max_doc_freq}\n",
    "\n",
    "    # отсортировать слова по убыванию частоты\n",
    "    sorted_word_counts = sorted(word_counts.items(),\n",
    "                                reverse=True,\n",
    "                                key=lambda pair: pair[1])\n",
    "\n",
    "    # добавим несуществующее слово с индексом 0 для удобства пакетной обработки\n",
    "    if pad_word is not None:\n",
    "        sorted_word_counts = [(pad_word, 0)] + sorted_word_counts\n",
    "\n",
    "    # если у нас по прежнему слишком много слов, оставить только max_size самых частотных\n",
    "    if len(word_counts) > max_size:\n",
    "        sorted_word_counts = sorted_word_counts[:max_size]\n",
    "\n",
    "    # нумеруем слова\n",
    "    word2id = {word: i for i, (word, _) in enumerate(sorted_word_counts)}\n",
    "\n",
    "    # нормируем частоты слов\n",
    "    word2freq = np.array([cnt / doc_n for _, cnt in sorted_word_counts], dtype='float32')\n",
    "\n",
    "    return word2id, word2freq\n",
    "\n",
    "\n",
    "PAD_TOKEN = '__PAD__'\n",
    "NUMERIC_TOKEN = '__NUMBER__'\n",
    "NUMERIC_RE = re.compile(r'^([0-9.,e+\\-]+|[mcxvi]+)$', re.I)\n",
    "\n",
    "\n",
    "def replace_number_nokens(tokenized_texts):\n",
    "    return [[token if not NUMERIC_RE.match(token) else NUMERIC_TOKEN for token in text]\n",
    "            for text in tokenized_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2fae2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "def vectorize_texts(tokenized_texts, word2id, word2freq, mode='tfidf', scale=True):\n",
    "    assert mode in {'tfidf', 'idf', 'tf', 'bin'}\n",
    "\n",
    "    # считаем количество употреблений каждого слова в каждом документе\n",
    "    result = scipy.sparse.dok_matrix((len(tokenized_texts), len(word2id)), dtype='float32')\n",
    "    for text_i, text in enumerate(tokenized_texts):\n",
    "        for token in text:\n",
    "            if token in word2id:\n",
    "                result[text_i, word2id[token]] += 1\n",
    "\n",
    "    # получаем бинарные вектора \"встречается или нет\"\n",
    "    if mode == 'bin':\n",
    "        result = (result > 0).astype('float32')\n",
    "\n",
    "    # получаем вектора относительных частот слова в документе\n",
    "    elif mode == 'tf':\n",
    "        result = result.tocsr()\n",
    "        result = result.multiply(1 / result.sum(1))\n",
    "\n",
    "    # полностью убираем информацию о количестве употреблений слова в данном документе,\n",
    "    # но оставляем информацию о частотности слова в корпусе в целом\n",
    "    elif mode == 'idf':\n",
    "        result = (result > 0).astype('float32').multiply(1 / word2freq)\n",
    "\n",
    "    # учитываем всю информацию, которая у нас есть:\n",
    "    # частоту слова в документе и частоту слова в корпусе\n",
    "    elif mode == 'tfidf':\n",
    "        result = result.tocsr()\n",
    "        result = result.multiply(1 / result.sum(1))  # разделить каждую строку на её длину\n",
    "        result = result.multiply(1 / word2freq)  # разделить каждый столбец на вес слова\n",
    "\n",
    "    if scale:\n",
    "        result = result.tocsc()\n",
    "        result -= result.min()\n",
    "        result /= (result.max() + 1e-6)\n",
    "\n",
    "    return result.tocsr()\n",
    "\n",
    "\n",
    "class SparseFeaturesDataset(Dataset):\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.features.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        cur_features = torch.from_numpy(self.features[idx].toarray()[0]).float()\n",
    "        cur_label = torch.from_numpy(np.asarray(self.targets[idx])).long()\n",
    "        return cur_features, cur_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca96d5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"data/bitkogan.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f3a98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([])\n",
    "for root, dirs, files in os.walk('data'):\n",
    "    for file in tqdm(files[:1]):\n",
    "        df1 = pd.read_csv(f'data/{file}')\n",
    "        df = pd.concat([df, df1], axis=0, ignore_index=True)\n",
    "\n",
    "df = df.sort_values('date', ignore_index=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c824404e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_start</th>\n",
       "      <th>pnl</th>\n",
       "      <th>pnl_sign</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-01-04</td>\n",
       "      <td>291264.097914</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-01-05</td>\n",
       "      <td>-411993.830320</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-01-06</td>\n",
       "      <td>521491.686795</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-01-08</td>\n",
       "      <td>62842.634116</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-01-11</td>\n",
       "      <td>-537598.706217</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>2021-12-20</td>\n",
       "      <td>-407527.554561</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>2021-12-21</td>\n",
       "      <td>-176881.417077</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>2021-12-22</td>\n",
       "      <td>-206943.414418</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>2021-12-23</td>\n",
       "      <td>23073.596468</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>2021-12-24</td>\n",
       "      <td>283960.438531</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     date_start            pnl  pnl_sign\n",
       "0    2021-01-04  291264.097914         1\n",
       "1    2021-01-05 -411993.830320         0\n",
       "2    2021-01-06  521491.686795         1\n",
       "3    2021-01-08   62842.634116         1\n",
       "4    2021-01-11 -537598.706217         0\n",
       "..          ...            ...       ...\n",
       "245  2021-12-20 -407527.554561         0\n",
       "246  2021-12-21 -176881.417077         0\n",
       "247  2021-12-22 -206943.414418         0\n",
       "248  2021-12-23   23073.596468         1\n",
       "249  2021-12-24  283960.438531         1\n",
       "\n",
       "[250 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pnl = pd.read_csv('Backtest_USDRUB_5_days.txt')\n",
    "pnl['date_start'] = pd.to_datetime(pnl['date_start']).dt.strftime('%Y-%m-%d')\n",
    "pnl['pnl_sign'] = pnl['pnl'].apply(lambda x: 1 if x >= 0 else 0)\n",
    "pnl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15a3bcd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.536"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pnl['pnl_sign'].sum() / pnl.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab142de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pnl['cumm_text'] = None\n",
    "pnl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2969bf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['date']).dt.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467d1f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_pnl = df[(df['date'] >= '2021-01-01') & (df['date'] <= '2021-12-31')]\n",
    "df_with_pnl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab208030",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "texts = ''\n",
    "for _, row in df_with_pnl.iterrows():\n",
    "    if i >= pnl.shape[0]:\n",
    "        break\n",
    "\n",
    "    if row['date'] <= pnl['date_start'][i]:\n",
    "        texts += ' ' + row['text']\n",
    "    else:\n",
    "        pnl.at[i, 'cumm_text'] = texts\n",
    "        texts = ''\n",
    "        i += 1\n",
    "        continue\n",
    "\n",
    "pnl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066a1be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, val = train_test_split(pnl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5050cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokenized = tokenize_corpus(train['cumm_text'])\n",
    "val_tokenized = tokenize_corpus(val['cumm_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515bcdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(' '.join(train_tokenized[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21450b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# строим словарь - vocabulary с помощью функции build_vocabulary\n",
    "# принимает на вход список списков токенезированные\n",
    "# word_doc_freq - содержит относительные частоты всех слов в датасете, он понадобиться \n",
    "# на этапе формирования матрицы признаков\n",
    "\n",
    "MAX_DF = 0.8 #во скольких документах встречаеться слово\n",
    "MIN_COUNT = 5 # сколько раз слово встречаеться в тексте\n",
    "\n",
    "\n",
    "vocabulary, word_doc_freq = build_vocabulary(train_tokenized, max_doc_freq=MAX_DF, min_count=MIN_COUNT)\n",
    "UNIQUE_WORDS_N = len(vocabulary)\n",
    "print('Количество уникальных токенов', UNIQUE_WORDS_N)\n",
    "print(list(vocabulary.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49502d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(word_doc_freq, bins=20)\n",
    "plt.title('Распределение относительных частот слов')\n",
    "plt.yscale('log');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adcfc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "VECTORIZATION_MODE = 'tfidf'\n",
    "# построение матрицы признаков по методу мешка слов\n",
    "# функция vectorize_texts принимает на вход\n",
    "#1. токенизированные список списков\n",
    "#2. словарь\n",
    "#3. вектор частоты токенизированны\n",
    "#4. алгоритм взвешивания токенов по частоте mode - есть 4 алгорима - bin,tf,idf,tfidf\n",
    "#5. флаг чтобы перемаштабировать флаг после взвешивания\n",
    "\n",
    "train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n",
    "\n",
    "print('Размерность матрицы признаков обучающей выборки', train_vectors.shape)\n",
    "print()\n",
    "print('Количество ненулевых элементов в обучающей выборке', train_vectors.nnz)\n",
    "print('Процент заполненности матрицы признаков {:.2f}%'.format(train_vectors.nnz * 100 / (train_vectors.shape[0] * train_vectors.shape[1])))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37703aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(train_vectors.data, bins=20)\n",
    "plt.title('Распределение весов признаков')\n",
    "plt.yscale('log');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c52193",
   "metadata": {},
   "outputs": [],
   "source": [
    "#word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0905a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b142cc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7c622e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "for k, v in api.info().items():\n",
    "    for k1, v1 in v.items():\n",
    "        print(k1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75a3b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_t1 = api.load('glove-twitter-200')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "499b448c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_t = api.load('glove-twitter-100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0d602e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import utils\n",
    "\n",
    "def get_vectors_gt100(row):\n",
    "    '''\n",
    "      word_doc_freq # частоты слов\n",
    "      train_tokenized #сами слова\n",
    "    '''\n",
    "    vecs = [np.zeros(100)]\n",
    "    for word in row:\n",
    "        #print(row)\n",
    "        try: \n",
    "            # если слово есть в нашем очищенном словаре\n",
    "            # умножаем вектор на вес tfidf\n",
    "            v = model_t[word] * word_doc_freq[vocabulary[word]] \n",
    "        except:\n",
    "            v = np.zeros(100)\n",
    "        vecs.append(v)\n",
    "    return np.sum(np.array(vecs),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681eaebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors_gt100 = np.array([get_vectors_gt100(i) for i in train_tokenized])\n",
    "val_vectors_gt100 = np.array([get_vectors_gt100(i) for i in val_tokenized])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc476d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors_gt100, train_vectors_gt100.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f199ed80",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_t1.most_similar(positive=['инвестор', 'рынок'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89f491d3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_t' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel_t\u001b[49m\u001b[38;5;241m.\u001b[39mmost_similar(positive\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mинвестор\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mрынок\u001b[39m\u001b[38;5;124m'\u001b[39m], topn\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_t' is not defined"
     ]
    }
   ],
   "source": [
    "model_t.most_similar(positive=['инвестор', 'рынок'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7de638",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train['pnl_sign']\n",
    "X_train = train_vectors_gt100\n",
    "\n",
    "y_val = val['pnl_sign']\n",
    "X_val = val_vectors_gt100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d15e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34231d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "print(model.score(X_val, y_val))\n",
    "print('данные:', list(os.walk('data/'))[0][-1][:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a468f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "print(model.score(X_val, y_val))\n",
    "print('данные:', list(os.walk('data/'))[0][-1][:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b180b4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import utils\n",
    "\n",
    "def get_vectors_gt100(row):\n",
    "    '''\n",
    "      word_doc_freq # частоты слов\n",
    "      train_tokenized #сами слова\n",
    "    '''\n",
    "    vecs = [np.zeros(100)]\n",
    "    for word in row:\n",
    "        #print(row)\n",
    "        try: \n",
    "            # если слово есть в нашем очищенном словаре\n",
    "            # умножаем вектор на вес tfidf\n",
    "            v = model_t[word] * word_doc_freq[vocabulary[word]] \n",
    "        except:\n",
    "            v = np.zeros(100)\n",
    "        vecs.append(v)\n",
    "    return np.sum(np.array(vecs),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4da54a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_t = api.load('glove-twitter-100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "462cf8c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|████                                        | 1/11 [00:02<00:22,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5238095238095238\n",
      "данные: ['cbonds.csv']\n",
      "всего наблюдений: 3618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|████████                                    | 2/11 [00:04<00:19,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5714285714285714\n",
      "данные: ['cbonds.csv', 'themovchans.csv']\n",
      "всего наблюдений: 3726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 27%|████████████                                | 3/11 [00:06<00:16,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.42857142857142855\n",
      "данные: ['cbonds.csv', 'themovchans.csv', 'headlines_QUANTS.csv']\n",
      "всего наблюдений: 3726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 36%|████████████████                            | 4/11 [00:08<00:14,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.42857142857142855\n",
      "данные: ['cbonds.csv', 'themovchans.csv', 'headlines_QUANTS.csv', 'War_Wealth_Wisdom.csv']\n",
      "всего наблюдений: 3726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|████████████████████                        | 5/11 [00:11<00:15,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4603174603174603\n",
      "данные: ['cbonds.csv', 'themovchans.csv', 'headlines_QUANTS.csv', 'War_Wealth_Wisdom.csv', 'mmi.csv']\n",
      "всего наблюдений: 7125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|████████████████████████                    | 6/11 [00:15<00:14,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4444444444444444\n",
      "данные: ['cbonds.csv', 'themovchans.csv', 'headlines_QUANTS.csv', 'War_Wealth_Wisdom.csv', 'mmi.csv', 'vts.csv']\n",
      "всего наблюдений: 7125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 64%|████████████████████████████                | 7/11 [00:20<00:15,  3.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49206349206349204\n",
      "данные: ['cbonds.csv', 'themovchans.csv', 'headlines_QUANTS.csv', 'War_Wealth_Wisdom.csv', 'mmi.csv', 'vts.csv', 'signal.csv']\n",
      "всего наблюдений: 20494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 73%|████████████████████████████████            | 8/11 [00:26<00:13,  4.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4444444444444444\n",
      "данные: ['cbonds.csv', 'themovchans.csv', 'headlines_QUANTS.csv', 'War_Wealth_Wisdom.csv', 'mmi.csv', 'vts.csv', 'signal.csv', '.gitignore']\n",
      "всего наблюдений: 20494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 82%|████████████████████████████████████        | 9/11 [00:32<00:09,  4.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38095238095238093\n",
      "данные: ['cbonds.csv', 'themovchans.csv', 'headlines_QUANTS.csv', 'War_Wealth_Wisdom.csv', 'mmi.csv', 'vts.csv', 'signal.csv', '.gitignore', 'zgmail.csv']\n",
      "всего наблюдений: 20494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 91%|███████████████████████████████████████    | 10/11 [00:38<00:05,  5.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49206349206349204\n",
      "данные: ['cbonds.csv', 'themovchans.csv', 'headlines_QUANTS.csv', 'War_Wealth_Wisdom.csv', 'mmi.csv', 'vts.csv', 'signal.csv', '.gitignore', 'zgmail.csv', 'rshb_invest.csv']\n",
      "всего наблюдений: 22043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 11/11 [00:44<00:00,  4.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47619047619047616\n",
      "данные: ['cbonds.csv', 'themovchans.csv', 'headlines_QUANTS.csv', 'War_Wealth_Wisdom.csv', 'mmi.csv', 'vts.csv', 'signal.csv', '.gitignore', 'zgmail.csv', 'rshb_invest.csv', 'Alfa_Wealth.csv']\n",
      "всего наблюдений: 22218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "input_path = 'data/' # нужно указать путь до папки с данными по новостям\n",
    "\n",
    "for root, dirs, files in os.walk(input_path):\n",
    "    for q in tqdm(range(1, 12)):\n",
    "        df = pd.DataFrame([])\n",
    "        for file in files[:q]:\n",
    "            df1 = pd.read_csv(f'{input_path}/{file}')\n",
    "            df = pd.concat([df, df1], axis=0, ignore_index=True)\n",
    "\n",
    "        df = df.sort_values('date', ignore_index=True)\n",
    "        df['date'] = pd.to_datetime(df['date']).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "        pnl = pd.read_csv('Backtest_USDRUB_5_days.txt')\n",
    "        pnl['date_start'] = pd.to_datetime(pnl['date_start']).dt.strftime('%Y-%m-%d')\n",
    "        pnl['pnl_sign'] = pnl['pnl'].apply(lambda x: 1 if x >= 0 else 0)\n",
    "        pnl['cumm_text'] = None\n",
    "\n",
    "        df_with_pnl = df[(df['date'] >= '2021-01-01') & (df['date'] <= '2021-12-31')]\n",
    "\n",
    "        i = 0\n",
    "        texts = ''\n",
    "        for _,row in df_with_pnl.iterrows():\n",
    "            if i >= pnl.shape[0]:\n",
    "                break\n",
    "\n",
    "            if row['date'] <= pnl['date_start'][i]:\n",
    "                texts += ' ' + row['text']\n",
    "            else:\n",
    "                pnl.at[i,'cumm_text'] = texts\n",
    "                texts = ''\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "        train, val = train_test_split(pnl)\n",
    "        train_tokenized = tokenize_corpus(train['cumm_text'])\n",
    "        val_tokenized = tokenize_corpus(val['cumm_text'])\n",
    "\n",
    "        MAX_DF = 0.8 #во скольких документах встречаеться слово\n",
    "        MIN_COUNT = 5 # сколько раз слово встречаеться в тексте\n",
    "\n",
    "\n",
    "        vocabulary, word_doc_freq = build_vocabulary(train_tokenized, max_doc_freq=MAX_DF, min_count=MIN_COUNT)\n",
    "        UNIQUE_WORDS_N = len(vocabulary)    \n",
    "        VECTORIZATION_MODE = 'tfidf'\n",
    "        train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n",
    "\n",
    "        train_vectors_gt100 = np.array([get_vectors_gt100(i) for i in train_tokenized])\n",
    "        val_vectors_gt100 = np.array([get_vectors_gt100(i) for i in val_tokenized])\n",
    "\n",
    "        model_t.most_similar(positive=['инвестор', 'рынок'], topn=1)\n",
    "\n",
    "        y_train = train['pnl_sign']\n",
    "        X_train = train_vectors_gt100\n",
    "\n",
    "        y_val = val['pnl_sign']\n",
    "        X_val = val_vectors_gt100\n",
    "\n",
    "        model = LogisticRegression()\n",
    "        model.fit(X_train, y_train)\n",
    "        print(model.score(X_val, y_val))\n",
    "        print('данные:', files[:q])\n",
    "        print('всего наблюдений:', df_with_pnl.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63e28619",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|████                                        | 1/11 [00:02<00:20,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.42857142857142855\n",
      "данные: ['cbonds.csv']\n",
      "всего наблюдений: 3618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|████████                                    | 2/11 [00:04<00:18,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5079365079365079\n",
      "данные: ['cbonds.csv', 'themovchans.csv']\n",
      "всего наблюдений: 3726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 27%|████████████                                | 3/11 [00:06<00:16,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4126984126984127\n",
      "данные: ['cbonds.csv', 'themovchans.csv', 'headlines_QUANTS.csv']\n",
      "всего наблюдений: 3726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 36%|████████████████                            | 4/11 [00:08<00:14,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4444444444444444\n",
      "данные: ['cbonds.csv', 'themovchans.csv', 'headlines_QUANTS.csv', 'War_Wealth_Wisdom.csv']\n",
      "всего наблюдений: 3726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|████████████████████                        | 5/11 [00:11<00:15,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5238095238095238\n",
      "данные: ['cbonds.csv', 'themovchans.csv', 'headlines_QUANTS.csv', 'War_Wealth_Wisdom.csv', 'mmi.csv']\n",
      "всего наблюдений: 7125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|████████████████████████                    | 6/11 [00:15<00:14,  2.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5079365079365079\n",
      "данные: ['cbonds.csv', 'themovchans.csv', 'headlines_QUANTS.csv', 'War_Wealth_Wisdom.csv', 'mmi.csv', 'vts.csv']\n",
      "всего наблюдений: 7125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 64%|████████████████████████████                | 7/11 [00:20<00:15,  3.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47619047619047616\n",
      "данные: ['cbonds.csv', 'themovchans.csv', 'headlines_QUANTS.csv', 'War_Wealth_Wisdom.csv', 'mmi.csv', 'vts.csv', 'signal.csv']\n",
      "всего наблюдений: 20494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 73%|████████████████████████████████            | 8/11 [00:26<00:13,  4.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5396825396825397\n",
      "данные: ['cbonds.csv', 'themovchans.csv', 'headlines_QUANTS.csv', 'War_Wealth_Wisdom.csv', 'mmi.csv', 'vts.csv', 'signal.csv', '.gitignore']\n",
      "всего наблюдений: 20494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 82%|████████████████████████████████████        | 9/11 [00:31<00:09,  4.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4126984126984127\n",
      "данные: ['cbonds.csv', 'themovchans.csv', 'headlines_QUANTS.csv', 'War_Wealth_Wisdom.csv', 'mmi.csv', 'vts.csv', 'signal.csv', '.gitignore', 'zgmail.csv']\n",
      "всего наблюдений: 20494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 91%|███████████████████████████████████████    | 10/11 [00:38<00:05,  5.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5873015873015873\n",
      "данные: ['cbonds.csv', 'themovchans.csv', 'headlines_QUANTS.csv', 'War_Wealth_Wisdom.csv', 'mmi.csv', 'vts.csv', 'signal.csv', '.gitignore', 'zgmail.csv', 'rshb_invest.csv']\n",
      "всего наблюдений: 22043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 11/11 [00:44<00:00,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.30158730158730157\n",
      "данные: ['cbonds.csv', 'themovchans.csv', 'headlines_QUANTS.csv', 'War_Wealth_Wisdom.csv', 'mmi.csv', 'vts.csv', 'signal.csv', '.gitignore', 'zgmail.csv', 'rshb_invest.csv', 'Alfa_Wealth.csv']\n",
      "всего наблюдений: 22218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "input_path = 'data/'\n",
    "\n",
    "for root, dirs, files in os.walk(input_path):\n",
    "    for q in tqdm(range(1, 12)):\n",
    "        df = pd.DataFrame([])\n",
    "        for file in files[:q]:\n",
    "            df1 = pd.read_csv(f'{input_path}/{file}')\n",
    "            df = pd.concat([df, df1], axis=0, ignore_index=True)\n",
    "\n",
    "        df = df.sort_values('date', ignore_index=True)\n",
    "        df['date'] = pd.to_datetime(df['date']).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "        pnl = pd.read_csv('Backtest_USDRUB_5_days.txt')\n",
    "        pnl['date_start'] = pd.to_datetime(pnl['date_start']).dt.strftime('%Y-%m-%d')\n",
    "        pnl['pnl_sign'] = pnl['pnl'].apply(lambda x: 1 if x >= 0 else 0)\n",
    "        pnl['cumm_text'] = None\n",
    "\n",
    "        df_with_pnl = df[(df['date'] >= '2021-01-01') & (df['date'] <= '2021-12-31')]\n",
    "\n",
    "        i = 0\n",
    "        texts = ''\n",
    "        for _,row in df_with_pnl.iterrows():\n",
    "            if i >= pnl.shape[0]:\n",
    "                break\n",
    "\n",
    "            if row['date'] <= pnl['date_start'][i]:\n",
    "                texts += ' ' + row['text']\n",
    "            else:\n",
    "                pnl.at[i,'cumm_text'] = texts\n",
    "                texts = ''\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "        train, val = train_test_split(pnl)\n",
    "        train_tokenized = tokenize_corpus(train['cumm_text'])\n",
    "        val_tokenized = tokenize_corpus(val['cumm_text'])\n",
    "\n",
    "        MAX_DF = 0.8 #во скольких документах встречаеться слово\n",
    "        MIN_COUNT = 5 # сколько раз слово встречаеться в тексте\n",
    "\n",
    "\n",
    "        vocabulary, word_doc_freq = build_vocabulary(train_tokenized, max_doc_freq=MAX_DF, min_count=MIN_COUNT)\n",
    "        UNIQUE_WORDS_N = len(vocabulary)    \n",
    "        VECTORIZATION_MODE = 'tfidf'\n",
    "        train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n",
    "\n",
    "        train_vectors_gt100 = np.array([get_vectors_gt100(i) for i in train_tokenized])\n",
    "        val_vectors_gt100 = np.array([get_vectors_gt100(i) for i in val_tokenized])\n",
    "\n",
    "        model_t.most_similar(positive=['инвестор', 'рынок'], topn=1)\n",
    "\n",
    "        y_train = train['pnl_sign']\n",
    "        X_train = train_vectors_gt100\n",
    "\n",
    "        y_val = val['pnl_sign']\n",
    "        X_val = val_vectors_gt100\n",
    "\n",
    "        model = LogisticRegression()\n",
    "        model.fit(X_train, y_train)\n",
    "        print(model.score(X_val, y_val))\n",
    "        print('данные:', files[:q])\n",
    "        print('всего наблюдений:', df_with_pnl.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3faf8196",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/11 [00:00<?, ?it/s]\n",
      "3572it [00:00, 44948.90it/s]\n",
      "  9%|████                                        | 1/11 [00:01<00:18,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6190476190476191\n",
      "данные: ['cbonds.csv']\n",
      "всего наблюдений: 3618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "3679it [00:00, 47984.32it/s]\n",
      " 18%|████████                                    | 2/11 [00:03<00:17,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.42857142857142855\n",
      "данные: ['cbonds.csv', 'themovchans.csv']\n",
      "всего наблюдений: 3726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "3679it [00:00, 43039.80it/s]\n",
      " 27%|████████████                                | 3/11 [00:06<00:16,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5079365079365079\n",
      "данные: ['cbonds.csv', 'themovchans.csv', 'headlines_QUANTS.csv']\n",
      "всего наблюдений: 3726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "3679it [00:00, 39035.39it/s]\n",
      " 36%|████████████████                            | 4/11 [00:08<00:14,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3968253968253968\n",
      "данные: ['cbonds.csv', 'themovchans.csv', 'headlines_QUANTS.csv', 'War_Wealth_Wisdom.csv']\n",
      "всего наблюдений: 3726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "7032it [00:00, 51098.03it/s]\u001b[A\n",
      " 45%|████████████████████                        | 5/11 [00:11<00:15,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5238095238095238\n",
      "данные: ['cbonds.csv', 'themovchans.csv', 'headlines_QUANTS.csv', 'War_Wealth_Wisdom.csv', 'mmi.csv']\n",
      "всего наблюдений: 7125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "7032it [00:00, 50523.48it/s]\u001b[A\n",
      " 55%|████████████████████████                    | 6/11 [00:15<00:14,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38095238095238093\n",
      "данные: ['cbonds.csv', 'themovchans.csv', 'headlines_QUANTS.csv', 'War_Wealth_Wisdom.csv', 'mmi.csv', 'vts.csv']\n",
      "всего наблюдений: 7125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "5319it [00:00, 53185.11it/s]\u001b[A\n",
      "10742it [00:00, 53797.49it/s]\u001b[A\n",
      "20258it [00:00, 53605.91it/s]\u001b[A\n",
      " 64%|████████████████████████████                | 7/11 [00:21<00:15,  3.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.42857142857142855\n",
      "данные: ['cbonds.csv', 'themovchans.csv', 'headlines_QUANTS.csv', 'War_Wealth_Wisdom.csv', 'mmi.csv', 'vts.csv', 'signal.csv']\n",
      "всего наблюдений: 20494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "5204it [00:00, 52031.24it/s]\u001b[A\n",
      "10581it [00:00, 53052.14it/s]\u001b[A\n",
      "20258it [00:00, 50506.75it/s]\u001b[A\n",
      " 73%|████████████████████████████████            | 8/11 [00:27<00:13,  4.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4603174603174603\n",
      "данные: ['cbonds.csv', 'themovchans.csv', 'headlines_QUANTS.csv', 'War_Wealth_Wisdom.csv', 'mmi.csv', 'vts.csv', 'signal.csv', '.gitignore']\n",
      "всего наблюдений: 20494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "5010it [00:00, 50087.63it/s]\u001b[A\n",
      "10019it [00:00, 49216.47it/s]\u001b[A\n",
      "14942it [00:00, 46853.30it/s]\u001b[A\n",
      "20258it [00:00, 46402.22it/s]\u001b[A\n",
      " 82%|████████████████████████████████████        | 9/11 [00:34<00:10,  5.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5238095238095238\n",
      "данные: ['cbonds.csv', 'themovchans.csv', 'headlines_QUANTS.csv', 'War_Wealth_Wisdom.csv', 'mmi.csv', 'vts.csv', 'signal.csv', '.gitignore', 'zgmail.csv']\n",
      "всего наблюдений: 20494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "5187it [00:00, 51866.34it/s]\u001b[A\n",
      "10641it [00:00, 53433.07it/s]\u001b[A\n",
      "16056it [00:00, 53756.75it/s]\u001b[A\n",
      "21791it [00:00, 53609.81it/s]\u001b[A\n",
      " 91%|███████████████████████████████████████    | 10/11 [00:40<00:05,  5.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5238095238095238\n",
      "данные: ['cbonds.csv', 'themovchans.csv', 'headlines_QUANTS.csv', 'War_Wealth_Wisdom.csv', 'mmi.csv', 'vts.csv', 'signal.csv', '.gitignore', 'zgmail.csv', 'rshb_invest.csv']\n",
      "всего наблюдений: 22043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "5210it [00:00, 52090.49it/s]\u001b[A\n",
      "10667it [00:00, 53547.07it/s]\u001b[A\n",
      "16061it [00:00, 53724.57it/s]\u001b[A\n",
      "21965it [00:00, 53616.15it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████| 11/11 [00:46<00:00,  4.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47619047619047616\n",
      "данные: ['cbonds.csv', 'themovchans.csv', 'headlines_QUANTS.csv', 'War_Wealth_Wisdom.csv', 'mmi.csv', 'vts.csv', 'signal.csv', '.gitignore', 'zgmail.csv', 'rshb_invest.csv', 'Alfa_Wealth.csv']\n",
      "всего наблюдений: 22218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "input_path = 'data/'\n",
    "\n",
    "for root, dirs, files in os.walk(input_path):\n",
    "    for q in tqdm(range(1, 12)):\n",
    "        df = pd.DataFrame([])\n",
    "        for file in files[:q]:\n",
    "            df1 = pd.read_csv(f'{input_path}/{file}')\n",
    "            df = pd.concat([df, df1], axis=0, ignore_index=True)\n",
    "\n",
    "        df = df.sort_values('date', ignore_index=True)\n",
    "        df['date'] = pd.to_datetime(df['date']).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "        pnl = pd.read_csv('Backtest_USDRUB_5_days.txt')\n",
    "        pnl['date_start'] = pd.to_datetime(pnl['date_start']).dt.strftime('%Y-%m-%d')\n",
    "        pnl['pnl_sign'] = pnl['pnl'].apply(lambda x: 1 if x >= 0 else 0)\n",
    "        pnl['cumm_text'] = None\n",
    "\n",
    "        df_with_pnl = df[(df['date'] >= '2021-01-01') & (df['date'] <= '2021-12-31')]\n",
    "\n",
    "        i = 0\n",
    "        texts = ''\n",
    "        for _,row in tqdm(df_with_pnl.iterrows()):\n",
    "            if i >= pnl.shape[0]:\n",
    "                break\n",
    "\n",
    "            if row['date'] <= pnl['date_start'][i]:\n",
    "                texts += ' ' + row['text']\n",
    "            else:\n",
    "                pnl.at[i,'cumm_text'] = texts\n",
    "                texts = ''\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "        train, val = train_test_split(pnl)\n",
    "        train_tokenized = tokenize_corpus(train['cumm_text'])\n",
    "        val_tokenized = tokenize_corpus(val['cumm_text'])\n",
    "\n",
    "        MAX_DF = 0.8 #во скольких документах встречаеться слово\n",
    "        MIN_COUNT = 5 # сколько раз слово встречаеться в тексте\n",
    "\n",
    "\n",
    "        vocabulary, word_doc_freq = build_vocabulary(train_tokenized, max_doc_freq=MAX_DF, min_count=MIN_COUNT)\n",
    "        UNIQUE_WORDS_N = len(vocabulary)    \n",
    "        VECTORIZATION_MODE = 'tfidf'\n",
    "        train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n",
    "\n",
    "        train_vectors_gt100 = np.array([get_vectors_gt100(i) for i in train_tokenized])\n",
    "        val_vectors_gt100 = np.array([get_vectors_gt100(i) for i in val_tokenized])\n",
    "\n",
    "        model_t.most_similar(positive=['инвестор', 'рынок'], topn=1)\n",
    "\n",
    "        y_train = train['pnl_sign']\n",
    "        X_train = train_vectors_gt100\n",
    "\n",
    "        y_val = val['pnl_sign']\n",
    "        X_val = val_vectors_gt100\n",
    "\n",
    "        model = LogisticRegression()\n",
    "        model.fit(X_train, y_train)\n",
    "        print(model.score(X_val, y_val))\n",
    "        print('данные:', files[:q])\n",
    "        print('всего наблюдений:', df_with_pnl.shape[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
